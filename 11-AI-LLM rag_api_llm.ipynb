{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6c6113",
   "metadata": {},
   "source": [
    "# PDF RAG 시스템 소스코드 분석\n",
    "\n",
    "## 📋 시스템 개요\n",
    "\n",
    "이 프로젝트는 **PDF 문서를 업로드하고 질문-답변을 수행하는 RAG(Retrieval-Augmented Generation) 시스템**입니다. FastAPI 기반으로 구축되었으며, Google Gemini API를 활용한 임베딩 생성과 답변 생성을 제공합니다.\n",
    "\n",
    "## 🏗️ 핵심 아키텍처\n",
    "\n",
    "### **계층화된 청킹 전략**\n",
    "시스템의 가장 혁신적인 부분은 **2단계 청킹 구조**입니다:\n",
    "\n",
    "```python\n",
    "class MainChunk:\n",
    "    \"\"\"구조화된 메인 청크 클래스\"\"\"\n",
    "    - 페이지 범위 기반의 큰 단위 청크\n",
    "    - 섹션/제목 정보 포함\n",
    "    - 여러 서브청크를 포함\n",
    "\n",
    "class SubChunk:\n",
    "    \"\"\"벡터화될 서브청크 클래스\"\"\"\n",
    "    - 2-3문장 단위의 작은 청크\n",
    "    - 실제 벡터 검색의 대상\n",
    "    - 부모 청크 참조 정보 보유\n",
    "```\n",
    "\n",
    "이 구조의 장점:\n",
    "- **정밀한 검색**: 서브청크로 정확한 매칭\n",
    "- **풍부한 컨텍스트**: 답변 생성 시 부모 청크의 전체 내용 활용\n",
    "- **의미적 일관성**: 섹션 단위 정보 보존\n",
    "\n",
    "## 🔍 하이브리드 검색 시스템\n",
    "\n",
    "### **벡터 검색 + BM25 검색 결합**\n",
    "\n",
    "```python\n",
    "async def hybrid_search(query: str, document_id: str = None, \n",
    "                       vector_weight: float = 0.7, \n",
    "                       keyword_weight: float = 0.3, \n",
    "                       top_k: int = 5):\n",
    "```\n",
    "\n",
    "**점수 계산 방식:**\n",
    "```python\n",
    "hybrid_score = (vector_weight * vector_score) + (keyword_weight * bm25_score)\n",
    "```\n",
    "\n",
    "- **벡터 검색 (70%)**: 의미적 유사성 기반\n",
    "- **BM25 검색 (30%)**: 키워드 매칭 기반\n",
    "- **가중치 조정 가능**: 도메인에 따른 최적화 가능\n",
    "\n",
    "## 📄 PDF 처리 파이프라인\n",
    "\n",
    "### **1단계: 텍스트 추출 및 구조화**\n",
    "\n",
    "```python\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"PDF에서 텍스트를 추출하고 페이지별로 구조화\"\"\"\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- PyMuPDF를 활용한 텍스트 추출\n",
    "- 휴리스틱 기반 섹션 제목 자동 인식\n",
    "- 페이지별 메타데이터 보존\n",
    "\n",
    "### **2단계: 메인 청크 생성**\n",
    "\n",
    "```python\n",
    "def create_main_chunks(pages_data: List[Dict[str, Any]]) -> List[MainChunk]:\n",
    "    \"\"\"페이지 데이터를 큰 구조화된 청크로 분할\"\"\"\n",
    "```\n",
    "\n",
    "**로직:**\n",
    "- 섹션 변화 지점에서 청크 분할\n",
    "- 페이지 범위 정보 유지\n",
    "- 최소 청크 수 보장 (페이지 단위 폴백)\n",
    "\n",
    "### **3단계: 서브청크 생성**\n",
    "\n",
    "```python\n",
    "def create_subchunks_from_main_chunk(main_chunk: MainChunk, \n",
    "                                   target_tokens: int = 150) -> List[SubChunk]:\n",
    "    \"\"\"메인 청크에서 2-3문장 단위의 서브청크 생성\"\"\"\n",
    "```\n",
    "\n",
    "**최적화된 분할 기준:**\n",
    "- 목표 토큰 수: 150개\n",
    "- 문장 수: 2-3문장\n",
    "- 토큰 오버플로우 방지\n",
    "- 문맥 경계 보존\n",
    "\n",
    "## 🤖 AI 모델 통합\n",
    "\n",
    "### **Gemini API 활용**\n",
    "\n",
    "```python\n",
    "# 임베딩 생성\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# 답변 생성\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "```\n",
    "\n",
    "**배치 처리 최적화:**\n",
    "- 30개 단위 배치 임베딩 생성\n",
    "- API 제한 고려한 처리량 조절\n",
    "- 비동기 처리로 성능 향상\n",
    "\n",
    "## 💾 데이터 저장 및 관리\n",
    "\n",
    "### **ChromaDB 벡터 저장소**\n",
    "\n",
    "```python\n",
    "collection.add(\n",
    "    documents=batch_texts,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadata_list,\n",
    "    ids=[f\"{document_id}_{subchunk.subchunk_id}\" for subchunk in batch_subchunks]\n",
    ")\n",
    "```\n",
    "\n",
    "**메타데이터 구조:**\n",
    "- `document_id`: 문서 고유 식별자\n",
    "- `parent_chunk_id`: 부모 청크 참조\n",
    "- `page_start/end`: 페이지 범위\n",
    "- `section/title`: 구조적 정보\n",
    "\n",
    "### **메모리 기반 보조 저장소**\n",
    "\n",
    "```python\n",
    "documents_registry = {}    # 문서 메타정보\n",
    "documents_chunks = {}      # 청크 데이터\n",
    "documents_bm25 = {}       # BM25 인덱스\n",
    "```\n",
    "\n",
    "## 🎯 질문-답변 프로세스\n",
    "\n",
    "### **컨텍스트 구성 전략**\n",
    "\n",
    "```python\n",
    "# 부모 청크의 전체 텍스트 활용\n",
    "context_part = f\"[{info['title']} - 페이지 {info['page_start']}\"\n",
    "context_part += f\", 섹션: {info['section']}]\\n{info['parent_text']}\"\n",
    "```\n",
    "\n",
    "**프롬프트 엔지니어링:**\n",
    "- 명확한 역할 정의\n",
    "- 문서 기반 답변 강제\n",
    "- 페이지/섹션 정보 포함 지시\n",
    "- 한국어 자연스러운 답변 요구\n",
    "\n",
    "## 🔧 API 엔드포인트\n",
    "\n",
    "### **핵심 기능**\n",
    "\n",
    "| 엔드포인트 | 기능 | 설명 |\n",
    "|-----------|------|------|\n",
    "| `POST /upload-pdf` | 문서 업로드 | PDF 처리 및 벡터화 |\n",
    "| `POST /question` | 질문 답변 | 하이브리드 검색 + LLM 생성 |\n",
    "| `GET /documents` | 문서 목록 | 등록된 모든 문서 조회 |\n",
    "| `DELETE /delete-document/{id}` | 문서 삭제 | 특정 문서 완전 삭제 |\n",
    "\n",
    "### **관리 기능**\n",
    "\n",
    "```python\n",
    "@app.get(\"/status\")\n",
    "async def get_status():\n",
    "    \"\"\"시스템 상태 모니터링\"\"\"\n",
    "    \n",
    "@app.delete(\"/delete-all\") \n",
    "async def delete_all_documents():\n",
    "    \"\"\"전체 데이터 초기화\"\"\"\n",
    "```\n",
    "\n",
    "## ⚡ 성능 최적화 요소\n",
    "\n",
    "### **배치 처리**\n",
    "- 임베딩 생성: 30개 단위 배치\n",
    "- API 호출 최소화\n",
    "- 메모리 효율성 고려\n",
    "\n",
    "### **비동기 처리**\n",
    "```python\n",
    "async def create_embeddings_batch(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"비동기 임베딩 생성\"\"\"\n",
    "```\n",
    "\n",
    "### **인덱스 최적화**\n",
    "- BM25 사전 계산 및 캐싱\n",
    "- ChromaDB 코사인 유사도 인덱스\n",
    "- 문서별 격리된 검색 지원\n",
    "\n",
    "## 🛠️ 주요 의존성\n",
    "\n",
    "```python\n",
    "# 핵심 프레임워크\n",
    "fastapi              # 웹 API 프레임워크\n",
    "pydantic            # 데이터 검증\n",
    "\n",
    "# PDF 처리\n",
    "pymupdf             # PDF 텍스트 추출\n",
    "\n",
    "# AI/ML\n",
    "langchain_google_genai  # Gemini API 래퍼\n",
    "tiktoken            # 토큰 카운팅\n",
    "\n",
    "# 벡터 저장소\n",
    "chromadb            # 벡터 데이터베이스\n",
    "\n",
    "# 검색 및 NLP\n",
    "rank_bm25           # BM25 알고리즘\n",
    "sklearn             # TF-IDF 벡터화\n",
    "```\n",
    "\n",
    "## 💡 시스템의 강점\n",
    "\n",
    "1. **계층화된 청킹**: 정밀 검색 + 풍부한 컨텍스트\n",
    "2. **하이브리드 검색**: 의미적 + 키워드 검색 결합\n",
    "3. **구조 보존**: PDF의 섹션/페이지 정보 활용\n",
    "4. **확장성**: 멀티 문서 지원 및 개별 관리\n",
    "5. **최적화**: 배치 처리 및 비동기 API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042776f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc242f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import pymupdf\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import tiktoken\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import uuid\n",
    "import tempfile\n",
    "import os\n",
    "import chromadb\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import time\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# 환경변수 로드\n",
    "load_dotenv()\n",
    "os.environ[\"CHROMA_TELEMETRY_ANONYMOUS\"] = \"False\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# 로깅 설정\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# chromadb telemetry 로그 레벨 WARNING 이상으로 격상(숨김)\n",
    "logging.getLogger(\"chromadb.telemetry\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.CRITICAL)\n",
    "\n",
    "app = FastAPI(title=\"PDF QA System (Gemini)\", version=\"1.0.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ChromaDB 클라이언트 초기화\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# 전역 변수들\n",
    "collection = None\n",
    "documents_registry = {}  # 문서 정보를 저장하는 레지스트리\n",
    "documents_chunks = {}  # 문서별 청크 데이터 저장\n",
    "documents_bm25 = {}  # 문서별 BM25 인덱스 저장\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 토크나이저(임베딩 토큰 카운트용)\n",
    "\n",
    "class QuestionRequest(BaseModel):\n",
    "    question: str\n",
    "    document_id: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3861a20",
   "metadata": {},
   "source": [
    "# 📚 2단계 청킹 전략: 책으로 이해하기\n",
    "\n",
    "## 🤔 왜 책을 챕터와 문단으로 나누어 읽을까요?\n",
    "\n",
    "책을 읽을 때 우리는 자연스럽게 **2단계로 정보를 처리**합니다:\n",
    "\n",
    "### 📖 일반적인 독서 과정\n",
    "```\n",
    "1단계: 목차를 보고 원하는 챕터 찾기\n",
    "       \"아, 마케팅 전략은 3챕터에 있구나!\"\n",
    "\n",
    "2단계: 챕터 안에서 구체적인 문단 찾기  \n",
    "       \"목표 고객에 대한 내용은 이 문단이네!\"\n",
    "\n",
    "3단계: 전체 챕터를 읽으며 맥락 이해\n",
    "       \"앞뒤 내용까지 읽어야 완전히 이해되겠다\"\n",
    "```\n",
    "\n",
    "RAG 시스템도 똑같은 방식으로 작동합니다!\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 MainChunk = 책의 챕터(Chapter)\n",
    "\n",
    "### 📖 실제 비즈니스 서적 예시\n",
    "\n",
    "```python\n",
    "class MainChunk:\n",
    "    \"\"\"책의 한 챕터\"\"\"\n",
    "    def __init__(self, text: str, chunk_id: str, page_start: int, page_end: int, \n",
    "                 section: str = \"\", title: str = \"\"):\n",
    "        self.text = text          # 📄 챕터 전체 내용\n",
    "        self.chunk_id = chunk_id  # 🏷️ 챕터 고유번호  \n",
    "        self.page_start = 15      # 📖 챕터 시작 페이지\n",
    "        self.page_end = 28        # 📖 챕터 끝 페이지  \n",
    "        self.section = \"제3장\"    # 📂 장 번호\n",
    "        self.title = \"디지털 마케팅 전략\"  # 📝 챕터 제목\n",
    "        self.subchunks = []       # 📝 이 챕터의 모든 문단들\n",
    "```\n",
    "\n",
    "### 📚 실제 챕터 내용 예시\n",
    "```\n",
    "📖 \"성공하는 스타트업의 마케팅\" 책의 제3장\n",
    "\n",
    "제3장: 디지털 마케팅 전략 (15-28페이지)\n",
    "├── 3.1 소셜미디어 마케팅\n",
    "├── 3.2 콘텐츠 마케팅 전략  \n",
    "├── 3.3 인플루언서 협업\n",
    "├── 3.4 광고 예산 배분\n",
    "└── 3.5 성과 측정 방법\n",
    "\n",
    "전체 텍스트: \"디지털 시대에 마케팅은 기업의 생존을 좌우합니다. \n",
    "소셜미디어는 고객과의 직접적인 소통 창구입니다... \n",
    "(14페이지 분량의 상세한 내용)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 SubChunk = 책의 문단(Paragraph)\n",
    "\n",
    "### ✏️ 실제 문단 예시\n",
    "\n",
    "```python\n",
    "class SubChunk:\n",
    "    \"\"\"책의 한 문단\"\"\"\n",
    "    def __init__(self, text: str, subchunk_id: str, parent_chunk_id: str, \n",
    "                 sentence_start: int, sentence_end: int):\n",
    "        self.text = text                 # 📝 문단 내용 (2-3문장)\n",
    "        self.subchunk_id = subchunk_id   # 🏷️ 문단 고유번호\n",
    "        self.parent_chunk_id = parent_chunk_id  # 📚 어느 챕터에 속하는지\n",
    "        self.sentence_start = sentence_start    # 📍 문단의 시작 문장 번호\n",
    "        self.sentence_end = sentence_end        # 📍 문단의 끝 문장 번호\n",
    "```\n",
    "\n",
    "### 📄 실제 문단들 예시\n",
    "\n",
    "**SubChunk 1: 인스타그램 마케팅**\n",
    "```python\n",
    "subchunk_1 = SubChunk(\n",
    "    text=\"인스타그램은 2030 여성 고객층에게 가장 효과적인 플랫폼입니다. \n",
    "          시각적 콘텐츠를 통해 브랜드 스토리를 전달할 수 있습니다. \n",
    "          해시태그 전략이 노출도를 크게 좌우합니다.\",\n",
    "    subchunk_id=\"sub_3_1_1\",\n",
    "    parent_chunk_id=\"chapter_3\",  # 제3장에 속함\n",
    "    sentence_start=1,\n",
    "    sentence_end=3\n",
    ")\n",
    "```\n",
    "\n",
    "**SubChunk 2: 유튜브 마케팅**\n",
    "```python\n",
    "subchunk_2 = SubChunk(\n",
    "    text=\"유튜브는 모든 연령대에서 강력한 영향력을 보입니다. \n",
    "          교육적 콘텐츠와 엔터테인먼트를 결합하면 효과가 극대화됩니다.\",\n",
    "    subchunk_id=\"sub_3_1_2\", \n",
    "    parent_chunk_id=\"chapter_3\",  # 같은 제3장에 속함\n",
    "    sentence_start=4,\n",
    "    sentence_end=5\n",
    ")\n",
    "```\n",
    "\n",
    "**SubChunk 3: 콘텐츠 기획**\n",
    "```python\n",
    "subchunk_3 = SubChunk(\n",
    "    text=\"콘텐츠 기획 시 고객 페르소나를 명확히 설정해야 합니다. \n",
    "          감정적 연결고리가 있는 스토리텔링이 핵심입니다.\",\n",
    "    subchunk_id=\"sub_3_2_1\",\n",
    "    parent_chunk_id=\"chapter_3\",  # 같은 제3장에 속함  \n",
    "    sentence_start=15,\n",
    "    sentence_end=16\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 실제 동작 과정: 독서하는 AI\n",
    "\n",
    "### 🙋‍♀️ 사용자 질문\n",
    "```\n",
    "\"인스타그램 마케팅에서 가장 중요한 것은 무엇인가요?\"\n",
    "```\n",
    "\n",
    "### 🔎 1단계: 책 전체에서 관련 문단 찾기 (SubChunk 검색)\n",
    "\n",
    "**AI가 모든 문단을 빠르게 훑어봅니다**\n",
    "```python\n",
    "검색 결과:\n",
    "1. subchunk_3_1_1: \"인스타그램은 2030 여성 고객층에게...\" (관련도: 95%)\n",
    "2. subchunk_5_2_3: \"인스타그램 광고비 책정 방법...\" (관련도: 78%)  \n",
    "3. subchunk_7_1_2: \"인스타그램 분석 툴 사용법...\" (관련도: 65%)\n",
    "```\n",
    "\n",
    "### 📖 2단계: 해당 챕터 전체 읽기 (MainChunk 활용)\n",
    "\n",
    "**가장 관련도 높은 문단이 속한 챕터를 전부 읽습니다**\n",
    "```python\n",
    "subchunk_3_1_1 → parent_chunk_id: \"chapter_3\"\n",
    "→ 제3장 \"디지털 마케팅 전략\" 전체 내용 (15-28페이지) 활용\n",
    "```\n",
    "\n",
    "### 📝 3단계: 맥락을 이해한 완전한 답변\n",
    "\n",
    "**AI가 답변할 때:**\n",
    "```\n",
    "✅ 좋은 답변:\n",
    "\"인스타그램 마케팅에서 가장 중요한 것은 해시태그 전략입니다. \n",
    "(제3장 디지털 마케팅 전략, 17페이지 참조)\n",
    "\n",
    "특히 2030 여성 고객층을 타겟으로 할 때 효과적이며, \n",
    "시각적 콘텐츠를 통한 브랜드 스토리텔링과 함께 \n",
    "해시태그 전략을 구성하면 노출도를 크게 높일 수 있습니다.\n",
    "\n",
    "또한 같은 장에서 언급하듯이, 콘텐츠 기획 시 \n",
    "고객 페르소나를 명확히 설정하고 감정적 연결고리가 있는 \n",
    "스토리텔링을 결합하면 더욱 효과적입니다.\"\n",
    "\n",
    "출처: 제3장 디지털 마케팅 전략 (15-28페이지)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 다른 예시: IT 기술서적\n",
    "\n",
    "### 📖 \"파이썬 웹개발 완벽가이드\" 책\n",
    "\n",
    "**MainChunk: 제7장 데이터베이스 연동**\n",
    "```python\n",
    "main_chunk = MainChunk(\n",
    "    text=\"데이터베이스는 웹 애플리케이션의 핵심입니다...(20페이지 분량)\",\n",
    "    chunk_id=\"chapter_7\", \n",
    "    page_start=89,\n",
    "    page_end=109,\n",
    "    section=\"제7장\",\n",
    "    title=\"데이터베이스 연동\",\n",
    "    subchunks=[sub_7_1, sub_7_2, sub_7_3, ...]\n",
    ")\n",
    "```\n",
    "\n",
    "**SubChunk들**\n",
    "```python\n",
    "# SQLAlchemy 설치 방법\n",
    "sub_7_1 = SubChunk(\n",
    "    text=\"SQLAlchemy는 pip install sqlalchemy로 설치합니다. \n",
    "          가상환경 사용을 강력히 권장합니다.\",\n",
    "    subchunk_id=\"sub_7_1\",\n",
    "    parent_chunk_id=\"chapter_7\"\n",
    ")\n",
    "\n",
    "# 데이터베이스 연결 설정  \n",
    "sub_7_2 = SubChunk(\n",
    "    text=\"데이터베이스 URL 형식은 'dialect://user:password@host/database'입니다. \n",
    "          환경변수를 사용하여 보안을 강화하세요.\",\n",
    "    subchunk_id=\"sub_7_2\", \n",
    "    parent_chunk_id=\"chapter_7\"\n",
    ")\n",
    "\n",
    "# 모델 정의 방법\n",
    "sub_7_3 = SubChunk(\n",
    "    text=\"SQLAlchemy 모델은 db.Model을 상속받아 정의합니다. \n",
    "          각 컬럼은 db.Column으로 선언하며 타입을 명시해야 합니다.\",\n",
    "    subchunk_id=\"sub_7_3\",\n",
    "    parent_chunk_id=\"chapter_7\"  \n",
    ")\n",
    "```\n",
    "\n",
    "### 🤖 질문과 답변 예시\n",
    "\n",
    "**질문:** \"SQLAlchemy 모델은 어떻게 정의하나요?\"\n",
    "\n",
    "**검색 과정:**\n",
    "```\n",
    "1. SubChunk 검색: sub_7_3이 가장 관련성 높음 (92% 매치)\n",
    "2. MainChunk 확장: chapter_7 전체 내용 활용\n",
    "3. 맥락 있는 답변 생성\n",
    "```\n",
    "\n",
    "**답변:**\n",
    "```\n",
    "SQLAlchemy 모델은 db.Model을 상속받아 정의합니다. \n",
    "각 컬럼은 db.Column으로 선언하며 타입을 명시해야 합니다.\n",
    "\n",
    "예를 들어:\n",
    "class User(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    username = db.Column(db.String(80), unique=True)\n",
    "\n",
    "모델 정의 전에 SQLAlchemy를 설치하고 (pip install sqlalchemy), \n",
    "데이터베이스 연결을 올바르게 설정해야 합니다. \n",
    "자세한 설치 및 설정 방법은 같은 장의 앞부분을 참조하세요.\n",
    "\n",
    "출처: 제7장 데이터베이스 연동 (89-109페이지)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 왜 이 방식이 효과적인가?\n",
    "\n",
    "### 🎯 정확한 검색 = 책의 색인 활용\n",
    "```python\n",
    "# 나쁜 예: 책 전체에서 찾기\n",
    "\"파이썬 책 500페이지 전체에서 SQLAlchemy 관련 내용 찾아줘\"\n",
    "→ 너무 광범위하고 부정확\n",
    "\n",
    "# 좋은 예: 문단 단위로 정확히 찾기  \n",
    "\"SQLAlchemy 모델 정의\" 문단을 정확히 발견\n",
    "→ 정확하고 구체적인 정보 획득\n",
    "```\n",
    "\n",
    "### 📖 풍부한 맥락 = 챕터 전체 읽기\n",
    "```python\n",
    "# 단편적 정보만 제공하는 경우\n",
    "\"db.Model을 상속받아 정의합니다.\"\n",
    "→ 설치 방법, 설정 방법을 모름\n",
    "\n",
    "# 챕터 전체 맥락을 활용하는 경우\n",
    "\"설치 → 설정 → 모델 정의 → 사용법\"의 전체 플로우 제공\n",
    "→ 완전하고 실용적인 답변\n",
    "```\n",
    "\n",
    "### 📍 출처 명확성 = 페이지 번호 제공\n",
    "```python\n",
    "return {\n",
    "    \"answer\": \"SQLAlchemy 모델은...\",\n",
    "    \"source\": \"제7장 데이터베이스 연동 (89-109페이지)\",\n",
    "    \"specific_section\": \"7.3 모델 정의\"\n",
    "}\n",
    "→ 사용자가 원본 책에서 추가 확인 가능\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 핵심 정리\n",
    "\n",
    "### 📚 MainChunk (챕터) = 숲을 보는 관점\n",
    "- **역할**: 전체적인 맥락과 구조 제공\n",
    "- **특징**: 한 주제에 대한 완전한 설명\n",
    "- **활용**: 답변 생성 시 풍부한 배경지식 제공\n",
    "\n",
    "### 📝 SubChunk (문단) = 나무를 보는 관점  \n",
    "- **역할**: 정확한 정보 검색\n",
    "- **특징**: 2-3문장의 구체적 내용\n",
    "- **활용**: 사용자 질문과의 정밀한 매칭\n",
    "\n",
    "### 🤝 협력 효과 = 완벽한 독서\n",
    "- **검색**: SubChunk로 정확한 문단 찾기\n",
    "- **이해**: MainChunk로 전체 맥락 파악  \n",
    "- **답변**: 정확하면서도 완전한 정보 제공\n",
    "\n",
    "**결과적으로 \"정확하면서도 완전한\" 답변을 만들어냅니다!** 📖✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MainChunk:\n",
    "    \"\"\"구조화된 메인 청크 클래스\"\"\"\n",
    "    def __init__(self, text: str, chunk_id: str, page_start: int, page_end: int, section: str = \"\", title: str = \"\"):\n",
    "        self.text = text\n",
    "        self.chunk_id = chunk_id\n",
    "        self.page_start = page_start\n",
    "        self.page_end = page_end\n",
    "        self.section = section\n",
    "        self.title = title\n",
    "        self.subchunks = []  # 이 청크에 속한 서브청크들\n",
    "\n",
    "class SubChunk:\n",
    "    \"\"\"벡터화될 서브청크 클래스\"\"\"\n",
    "    def __init__(self, text: str, subchunk_id: str, parent_chunk_id: str, sentence_start: int, sentence_end: int):\n",
    "        self.text = text\n",
    "        self.subchunk_id = subchunk_id\n",
    "        self.parent_chunk_id = parent_chunk_id\n",
    "        self.sentence_start = sentence_start\n",
    "        self.sentence_end = sentence_end\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"텍스트의 토큰 수를 계산합니다.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"PDF에서 텍스트를 추출하고 페이지별로 구조화합니다.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_data = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # 목차나 섹션 제목 추출 (개선된 휴리스틱)\n",
    "        lines = text.split('\\n')\n",
    "        section_title = \"\"\n",
    "        \n",
    "        # 제목 패턴 찾기 (숫자. 제목, 대문자 제목, 등)\n",
    "        for line in lines[:10]:  # 상위 10줄 확인\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # 숫자로 시작하는 제목 (1. 제목, 1.1 제목 등)\n",
    "                if re.match(r'^\\d+\\.?\\d*\\.?\\s+[A-Za-z가-힣]', line):\n",
    "                    section_title = line\n",
    "                    break\n",
    "                # 전체 대문자 제목\n",
    "                elif line.isupper() and 5 <= len(line) <= 50:\n",
    "                    section_title = line\n",
    "                    break\n",
    "                # 첫 글자만 대문자이고 적절한 길이\n",
    "                elif line[0].isupper() and 10 <= len(line) <= 80 and line.count(' ') <= 8:\n",
    "                    section_title = line\n",
    "                    break\n",
    "        \n",
    "        pages_data.append({\n",
    "            'page_num': page_num + 1,\n",
    "            'text': text,\n",
    "            'section': section_title\n",
    "        })\n",
    "    \n",
    "    doc.close()\n",
    "    return pages_data\n",
    "\n",
    "def create_main_chunks(pages_data: List[Dict[str, Any]]) -> List[MainChunk]:\n",
    "    \"\"\"페이지 데이터를 큰 구조화된 청크로 나눕니다.\"\"\"\n",
    "    chunks = []\n",
    "    current_section = \"\"\n",
    "    current_chunk_text = \"\"\n",
    "    current_pages = []\n",
    "    section_start_page = 1\n",
    "    \n",
    "    for i, page_data in enumerate(pages_data):\n",
    "        page_num = page_data['page_num']\n",
    "        text = page_data['text']\n",
    "        section = page_data['section']\n",
    "        \n",
    "        # 새로운 섹션이 시작되면 이전 청크를 완성\n",
    "        if section and section != current_section and current_chunk_text:\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            chunks.append(MainChunk(\n",
    "                text=current_chunk_text.strip(),\n",
    "                chunk_id=chunk_id,\n",
    "                page_start=section_start_page,\n",
    "                page_end=current_pages[-1] if current_pages else section_start_page,\n",
    "                section=current_section,\n",
    "                title=current_section\n",
    "            ))\n",
    "            \n",
    "            # 새 청크 시작\n",
    "            current_chunk_text = text\n",
    "            current_section = section\n",
    "            current_pages = [page_num]\n",
    "            section_start_page = page_num\n",
    "        else:\n",
    "            # 기존 청크에 페이지 추가\n",
    "            current_chunk_text += \"\\n\\n\" + text if current_chunk_text else text\n",
    "            current_pages.append(page_num)\n",
    "            if section and not current_section:\n",
    "                current_section = section\n",
    "                section_start_page = page_num\n",
    "    \n",
    "    # 마지막 청크 추가\n",
    "    if current_chunk_text.strip():\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        chunks.append(MainChunk(\n",
    "            text=current_chunk_text.strip(),\n",
    "            chunk_id=chunk_id,\n",
    "            page_start=section_start_page,\n",
    "            page_end=current_pages[-1] if current_pages else section_start_page,\n",
    "            section=current_section,\n",
    "            title=current_section\n",
    "        ))\n",
    "    \n",
    "    # 청크가 너무 적으면 페이지 단위로 분할\n",
    "    if len(chunks) < 3:\n",
    "        chunks = []\n",
    "        for page_data in pages_data:\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            chunks.append(MainChunk(\n",
    "                text=page_data['text'],\n",
    "                chunk_id=chunk_id,\n",
    "                page_start=page_data['page_num'],\n",
    "                page_end=page_data['page_num'],\n",
    "                section=page_data['section'],\n",
    "                title=f\"페이지 {page_data['page_num']}\"\n",
    "            ))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_subchunks_from_main_chunk(main_chunk: MainChunk, target_tokens: int = 150) -> List[SubChunk]:\n",
    "    \"\"\"메인 청크에서 2-3문장 단위의 서브청크를 생성합니다.\"\"\"\n",
    "    subchunks = []\n",
    "    text = main_chunk.text\n",
    "    \n",
    "    # 문장 단위로 분할 (개선된 정규식)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z가-힣])', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    current_subchunk = \"\"\n",
    "    current_tokens = 0\n",
    "    sentence_start_idx = 0\n",
    "    sentences_in_subchunk = 0\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        # 2-3문장이거나 토큰 수가 목표에 도달하면 서브청크 생성\n",
    "        should_create_subchunk = (\n",
    "            (sentences_in_subchunk >= 2 and current_tokens + sentence_tokens > target_tokens) or\n",
    "            sentences_in_subchunk >= 3 or\n",
    "            (current_tokens + sentence_tokens > target_tokens * 1.5 and sentences_in_subchunk >= 1)\n",
    "        )\n",
    "        \n",
    "        if should_create_subchunk and current_subchunk:\n",
    "            subchunk_id = str(uuid.uuid4())\n",
    "            subchunks.append(SubChunk(\n",
    "                text=current_subchunk.strip(),\n",
    "                subchunk_id=subchunk_id,\n",
    "                parent_chunk_id=main_chunk.chunk_id,\n",
    "                sentence_start=sentence_start_idx,\n",
    "                sentence_end=i - 1\n",
    "            ))\n",
    "            \n",
    "            # 새 서브청크 시작\n",
    "            current_subchunk = sentence\n",
    "            current_tokens = sentence_tokens\n",
    "            sentence_start_idx = i\n",
    "            sentences_in_subchunk = 1\n",
    "        else:\n",
    "            # 기존 서브청크에 문장 추가\n",
    "            current_subchunk += \" \" + sentence if current_subchunk else sentence\n",
    "            current_tokens += sentence_tokens\n",
    "            sentences_in_subchunk += 1\n",
    "    \n",
    "    # 마지막 서브청크 추가\n",
    "    if current_subchunk.strip():\n",
    "        subchunk_id = str(uuid.uuid4())\n",
    "        subchunks.append(SubChunk(\n",
    "            text=current_subchunk.strip(),\n",
    "            subchunk_id=subchunk_id,\n",
    "            parent_chunk_id=main_chunk.chunk_id,\n",
    "            sentence_start=sentence_start_idx,\n",
    "            sentence_end=len(sentences) - 1\n",
    "        ))\n",
    "    \n",
    "    return subchunks\n",
    "\n",
    "async def create_embeddings_batch(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"텍스트 배치에 대한 Gemini 임베딩을 생성합니다.\"\"\"\n",
    "    try:\n",
    "        # Gemini 임베딩 생성 (langchain 사용)\n",
    "        from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "        embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "        embeddings = embeddings_model.embed_documents(texts)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"임베딩 생성 중 오류: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"임베딩 생성 실패: {str(e)}\")\n",
    "\n",
    "def create_bm25_index(subchunks: List[SubChunk]) -> BM25Okapi:\n",
    "    \"\"\"서브청크들에 대한 BM25 인덱스를 생성합니다.\"\"\"\n",
    "    tokenized_subchunks = []\n",
    "    for subchunk in subchunks:\n",
    "        tokens = re.findall(r'\\b\\w+\\b', subchunk.text.lower())\n",
    "        tokenized_subchunks.append(tokens)\n",
    "    return BM25Okapi(tokenized_subchunks)\n",
    "\n",
    "\n",
    "@app.post(\"/upload-pdf\")\n",
    "async def upload_pdf(file: UploadFile = File(...)):\n",
    "    \"\"\"PDF 파일을 업로드하고 처리합니다.\"\"\"\n",
    "    global collection, documents_registry, documents_chunks, documents_bm25\n",
    "    \n",
    "    if not file.filename.endswith('.pdf'):\n",
    "        raise HTTPException(status_code=400, detail=\"PDF 파일만 업로드 가능합니다.\")\n",
    "    \n",
    "    try:\n",
    "        # 임시 파일로 저장\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "            content = await file.read()\n",
    "            tmp_file.write(content)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        # PDF에서 텍스트 추출\n",
    "        logger.info(\"PDF에서 텍스트 추출 중...\")\n",
    "        pages_data = extract_text_from_pdf(tmp_file_path)\n",
    "        \n",
    "        # 메인 청크 생성 (구조화된 큰 청크)\n",
    "        logger.info(\"구조화된 메인 청크 생성 중...\")\n",
    "        main_chunks = create_main_chunks(pages_data)\n",
    "        \n",
    "        if not main_chunks:\n",
    "            raise HTTPException(status_code=400, detail=\"PDF에서 텍스트를 추출할 수 없습니다.\")\n",
    "        \n",
    "        # 각 메인 청크에서 서브청크 생성\n",
    "        logger.info(\"서브청크 생성 중...\")\n",
    "        all_subchunks = []\n",
    "        for main_chunk in main_chunks:\n",
    "            subchunks = create_subchunks_from_main_chunk(main_chunk, target_tokens=150)\n",
    "            main_chunk.subchunks = subchunks\n",
    "            all_subchunks.extend(subchunks)\n",
    "        \n",
    "        if not all_subchunks:\n",
    "            raise HTTPException(status_code=400, detail=\"서브청크를 생성할 수 없습니다.\")\n",
    "        \n",
    "        # 문서 ID 생성\n",
    "        document_id = f\"doc_{uuid.uuid4().hex[:8]}_{int(time.time())}\"\n",
    "        \n",
    "        # 문서별 데이터 저장\n",
    "        documents_chunks[document_id] = {\n",
    "            'main_chunks': main_chunks,\n",
    "            'subchunks': all_subchunks\n",
    "        }\n",
    "        \n",
    "        # BM25 인덱스 생성 (해당 문서의 서브청크만)\n",
    "        logger.info(\"BM25 인덱스 생성 중...\")\n",
    "        documents_bm25[document_id] = create_bm25_index(all_subchunks)\n",
    "        \n",
    "        # ChromaDB 컬렉션 생성/업데이트\n",
    "        collection_name = \"rag\"  # 기본 컬렉션 이름\n",
    "        \n",
    "        try:\n",
    "            collection = chroma_client.get_collection(collection_name)\n",
    "        except:\n",
    "            collection = chroma_client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        \n",
    "        # 서브청크 임베딩 생성 및 저장 (배치 처리)\n",
    "        logger.info(\"서브청크 임베딩 생성 및 벡터 데이터베이스에 저장 중...\")\n",
    "        batch_size = 30  # OpenAI API 제한을 고려한 배치 크기\n",
    "        \n",
    "        for i in range(0, len(all_subchunks), batch_size):\n",
    "            batch_subchunks = all_subchunks[i:i + batch_size]\n",
    "            batch_texts = [subchunk.text for subchunk in batch_subchunks]\n",
    "            \n",
    "            # 임베딩 생성\n",
    "            embeddings = await create_embeddings_batch(batch_texts)\n",
    "            \n",
    "            # 각 서브청크의 부모 청크 정보 찾기\n",
    "            metadata_list = []\n",
    "            for subchunk in batch_subchunks:\n",
    "                parent_chunk = next((chunk for chunk in main_chunks if chunk.chunk_id == subchunk.parent_chunk_id), None)\n",
    "                metadata_list.append({\n",
    "                    \"document_id\": document_id,\n",
    "                    \"subchunk_id\": subchunk.subchunk_id,\n",
    "                    \"parent_chunk_id\": subchunk.parent_chunk_id,\n",
    "                    \"parent_section\": parent_chunk.section if parent_chunk else \"\",\n",
    "                    \"parent_title\": parent_chunk.title if parent_chunk else \"\",\n",
    "                    \"page_start\": parent_chunk.page_start if parent_chunk else 0,\n",
    "                    \"page_end\": parent_chunk.page_end if parent_chunk else 0,\n",
    "                    \"sentence_start\": subchunk.sentence_start,\n",
    "                    \"sentence_end\": subchunk.sentence_end\n",
    "                })\n",
    "            \n",
    "            # ChromaDB에 저장 (문서별 고유 ID 사용)\n",
    "            collection.add(\n",
    "                documents=batch_texts,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadata_list,\n",
    "                ids=[f\"{document_id}_{subchunk.subchunk_id}\" for subchunk in batch_subchunks]\n",
    "            )\n",
    "        \n",
    "        # 문서 정보를 레지스트리에 저장\n",
    "        documents_registry[document_id] = {\n",
    "            \"document_id\": document_id,\n",
    "            \"filename\": file.filename,\n",
    "            \"upload_time\": time.time(),\n",
    "            \"main_chunks_count\": len(main_chunks),\n",
    "            \"subchunks_count\": len(all_subchunks),\n",
    "            \"total_pages\": len(pages_data),\n",
    "            \"chunks_info\": [\n",
    "                {\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"title\": chunk.title,\n",
    "                    \"section\": chunk.section,\n",
    "                    \"pages\": f\"{chunk.page_start}-{chunk.page_end}\",\n",
    "                    \"subchunks_count\": len(chunk.subchunks),\n",
    "                    \"text_preview\": chunk.text[:200] + \"...\" if len(chunk.text) > 200 else chunk.text\n",
    "                }\n",
    "                for chunk in main_chunks\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # 임시 파일 삭제\n",
    "        os.unlink(tmp_file_path)\n",
    "        \n",
    "        logger.info(f\"PDF 처리 완료: {len(main_chunks)}개 메인청크, {len(all_subchunks)}개 서브청크 생성\")\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"message\": \"PDF 업로드 및 처리 완료\",\n",
    "            \"document_id\": document_id,\n",
    "            \"collection_name\": collection_name,\n",
    "            \"main_chunks_count\": len(main_chunks),\n",
    "            \"subchunks_count\": len(all_subchunks),\n",
    "            \"total_pages\": len(pages_data),\n",
    "            \"chunks_info\": documents_registry[document_id][\"chunks_info\"]\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"PDF 처리 중 오류: {e}\")\n",
    "        if 'tmp_file_path' in locals():\n",
    "            try:\n",
    "                os.unlink(tmp_file_path)\n",
    "            except:\n",
    "                pass\n",
    "        raise HTTPException(status_code=500, detail=f\"PDF 처리 실패: {str(e)}\")\n",
    "\n",
    "async def hybrid_search(query: str, document_id: str = None, vector_weight: float = 0.7, keyword_weight: float = 0.3, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"하이브리드 검색 (벡터 + 키워드)를 수행합니다.\"\"\"\n",
    "    if not collection:\n",
    "        raise HTTPException(status_code=400, detail=\"먼저 PDF를 업로드해주세요.\")\n",
    "    \n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = await create_embeddings_batch([query])\n",
    "    query_embedding = query_embedding[0]\n",
    "    \n",
    "    # 벡터 검색 (ChromaDB에서 직접)\n",
    "    if document_id:\n",
    "        # 특정 문서에서만 검색\n",
    "        vector_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=min(top_k * 3, 100),\n",
    "            where={\"document_id\": document_id}\n",
    "        )\n",
    "    else:\n",
    "        # 모든 문서에서 검색\n",
    "        vector_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=min(top_k * 3, 100)\n",
    "        )\n",
    "    \n",
    "    if not vector_results['ids'][0]:\n",
    "        return []\n",
    "    \n",
    "    # BM25 검색을 위한 준비\n",
    "    query_tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    \n",
    "    # 검색 결과 처리\n",
    "    search_results = []\n",
    "    \n",
    "    for i, (vector_id, distance, metadata, document_text) in enumerate(zip(\n",
    "        vector_results['ids'][0],\n",
    "        vector_results['distances'][0], \n",
    "        vector_results['metadatas'][0],\n",
    "        vector_results['documents'][0]\n",
    "    )):\n",
    "        # 벡터 점수 (거리를 유사도로 변환)\n",
    "        vector_score = 1 - distance\n",
    "        \n",
    "        # BM25 점수 계산 (해당 문서의 BM25 인덱스 사용)\n",
    "        doc_id = metadata.get('document_id', '')\n",
    "        bm25_score = 0.0\n",
    "        \n",
    "        if doc_id in documents_bm25 and document_text:\n",
    "            # 문서 텍스트를 토큰화\n",
    "            doc_tokens = re.findall(r'\\b\\w+\\b', document_text.lower())\n",
    "            \n",
    "            # BM25 점수 계산 (간단한 TF-IDF 기반)\n",
    "            query_term_scores = []\n",
    "            for term in query_tokens:\n",
    "                if term in doc_tokens:\n",
    "                    tf = doc_tokens.count(term)\n",
    "                    # 간단한 BM25 근사\n",
    "                    query_term_scores.append(tf / (tf + 1.0))\n",
    "            \n",
    "            if query_term_scores:\n",
    "                bm25_score = sum(query_term_scores) / len(query_term_scores)\n",
    "        \n",
    "        # 하이브리드 점수 계산\n",
    "        hybrid_score = (vector_weight * vector_score) + (keyword_weight * bm25_score)\n",
    "        \n",
    "        # 부모 청크 정보 찾기\n",
    "        parent_chunk = None\n",
    "        if doc_id in documents_chunks:\n",
    "            parent_chunk_id = metadata.get('parent_chunk_id', '')\n",
    "            for chunk in documents_chunks[doc_id]['main_chunks']:\n",
    "                if chunk.chunk_id == parent_chunk_id:\n",
    "                    parent_chunk = chunk\n",
    "                    break\n",
    "        \n",
    "        # 서브청크 정보 찾기\n",
    "        subchunk = None\n",
    "        if doc_id in documents_chunks:\n",
    "            subchunk_id = metadata.get('subchunk_id', '')\n",
    "            for sc in documents_chunks[doc_id]['subchunks']:\n",
    "                if sc.subchunk_id == subchunk_id:\n",
    "                    subchunk = sc\n",
    "                    break\n",
    "        \n",
    "        # 서브청크가 없으면 임시로 생성\n",
    "        if not subchunk:\n",
    "            subchunk = SubChunk(\n",
    "                text=document_text,\n",
    "                subchunk_id=metadata.get('subchunk_id', f'temp_{i}'),\n",
    "                parent_chunk_id=metadata.get('parent_chunk_id', ''),\n",
    "                sentence_start=0,\n",
    "                sentence_end=0\n",
    "            )\n",
    "        \n",
    "        search_results.append({\n",
    "            'subchunk': subchunk,\n",
    "            'parent_chunk': parent_chunk,\n",
    "            'score': hybrid_score,\n",
    "            'vector_score': vector_score,\n",
    "            'bm25_score': bm25_score,\n",
    "            'document_id': doc_id,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    \n",
    "    # 점수 기준으로 정렬\n",
    "    search_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return search_results[:top_k]\n",
    "\n",
    "@app.post(\"/question\")\n",
    "async def ask_question(request: QuestionRequest):\n",
    "    \"\"\"사용자 질문에 답변합니다.\"\"\"\n",
    "    try:\n",
    "        # 하이브리드 검색 수행\n",
    "        search_results = await hybrid_search(request.question, request.document_id, top_k=5)\n",
    "        \n",
    "        if not search_results:\n",
    "            raise HTTPException(status_code=404, detail=\"관련 정보를 찾을 수 없습니다.\")\n",
    "        \n",
    "        # 검색된 서브청크들과 해당하는 부모 청크 정보 수집\n",
    "        context_info = []\n",
    "        used_parent_chunks = set()\n",
    "        \n",
    "        for result in search_results:\n",
    "            subchunk = result['subchunk']\n",
    "            parent_chunk = result['parent_chunk']\n",
    "            \n",
    "            # 부모 청크의 전체 컨텍스트 사용\n",
    "            if parent_chunk and parent_chunk.chunk_id not in used_parent_chunks:\n",
    "                context_info.append({\n",
    "                    'parent_text': parent_chunk.text,\n",
    "                    'subchunk_text': subchunk.text,\n",
    "                    'title': parent_chunk.title,\n",
    "                    'section': parent_chunk.section,\n",
    "                    'page_start': parent_chunk.page_start,\n",
    "                    'page_end': parent_chunk.page_end,\n",
    "                    'score': result['score'],\n",
    "                    'document_id': result['document_id']\n",
    "                })\n",
    "                used_parent_chunks.add(parent_chunk.chunk_id)\n",
    "            elif not parent_chunk:\n",
    "                # 부모 청크가 없는 경우 서브청크만 사용\n",
    "                context_info.append({\n",
    "                    'parent_text': subchunk.text,\n",
    "                    'subchunk_text': subchunk.text,\n",
    "                    'title': result['metadata'].get('parent_title', '제목 없음'),\n",
    "                    'section': result['metadata'].get('parent_section', '섹션 없음'),\n",
    "                    'page_start': result['metadata'].get('page_start', 0),\n",
    "                    'page_end': result['metadata'].get('page_end', 0),\n",
    "                    'score': result['score'],\n",
    "                    'document_id': result['document_id']\n",
    "                })\n",
    "        \n",
    "        # 컨텍스트 구성 (부모 청크의 전체 텍스트 사용)\n",
    "        context_parts = []\n",
    "        for info in context_info:\n",
    "            context_part = f\"[{info['title']} - 페이지 {info['page_start']}\"\n",
    "            if info['page_end'] != info['page_start']:\n",
    "                context_part += f\"-{info['page_end']}\"\n",
    "            context_part += f\", 섹션: {info['section']}]\\n{info['parent_text']}\"\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        context = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # LLM 프롬프트 구성\n",
    "        prompt = f\"\"\"당신은 업로드된 PDF 문서를 기반으로 질문에 답변하는 AI 어시스턴트입니다.\n",
    "\n",
    "다음 문서의 관련 섹션들을 참고하여 사용자의 질문에 정확하고 도움이 되는 답변을 제공해주세요:\n",
    "\n",
    "=== 관련 문서 섹션들 ===\n",
    "{context}\n",
    "\n",
    "=== 사용자 질문 ===\n",
    "{request.question}\n",
    "\n",
    "=== 답변 지침 ===\n",
    "1. 제공된 문서 내용만을 기반으로 답변하세요\n",
    "2. 문서에 없는 내용은 추측하지 마세요\n",
    "3. 관련 섹션과 페이지 번호를 언급해주세요\n",
    "4. 여러 섹션의 정보를 종합하여 답변하세요\n",
    "5. 답변이 불분명하다면 그 이유를 설명해주세요\n",
    "6. 한국어로 자연스럽고 이해하기 쉽게 답변해주세요\n",
    "\n",
    "답변:\"\"\"\n",
    "\n",
    "        # Gemini API 호출\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"question\": request.question,\n",
    "            \"answer\": answer,\n",
    "            \"context_sources\": [\n",
    "                {\n",
    "                    \"document_id\": info['document_id'],\n",
    "                    \"title\": info['title'],\n",
    "                    \"section\": info['section'],\n",
    "                    \"pages\": f\"{info['page_start']}-{info['page_end']}\" if info['page_end'] != info['page_start'] else str(info['page_start']),\n",
    "                    \"score\": round(info['score'], 3),\n",
    "                    \"matched_subchunk\": info['subchunk_text'][:200] + \"...\" if len(info['subchunk_text']) > 200 else info['subchunk_text'],\n",
    "                    \"full_section_preview\": info['parent_text'][:300] + \"...\" if len(info['parent_text']) > 300 else info['parent_text']\n",
    "                }\n",
    "                for info in context_info\n",
    "            ],\n",
    "            \"total_sections_found\": len(context_info)\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"질문 처리 중 오류: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"질문 처리 실패: {str(e)}\")\n",
    "\n",
    "@app.delete(\"/delete-document/{document_id}\")\n",
    "async def delete_document(document_id: str):\n",
    "    \"\"\"특정 문서와 관련된 데이터를 삭제합니다.\"\"\"\n",
    "    global collection, documents_registry, documents_chunks, documents_bm25\n",
    "    \n",
    "    try:\n",
    "        deleted_items = []\n",
    "        \n",
    "        # 1. 문서 레지스트리에서 삭제\n",
    "        if document_id in documents_registry:\n",
    "            del documents_registry[document_id]\n",
    "            deleted_items.append(\"문서 레지스트리 정보\")\n",
    "        \n",
    "        # 2. 문서별 청크 데이터 삭제\n",
    "        if document_id in documents_chunks:\n",
    "            chunk_count = len(documents_chunks[document_id]['main_chunks'])\n",
    "            subchunk_count = len(documents_chunks[document_id]['subchunks'])\n",
    "            del documents_chunks[document_id]\n",
    "            deleted_items.append(f\"메인 청크 데이터 ({chunk_count}개)\")\n",
    "            deleted_items.append(f\"서브 청크 데이터 ({subchunk_count}개)\")\n",
    "        \n",
    "        # 3. 문서별 BM25 인덱스 삭제\n",
    "        if document_id in documents_bm25:\n",
    "            del documents_bm25[document_id]\n",
    "            deleted_items.append(\"BM25 인덱스\")\n",
    "        \n",
    "        # 4. ChromaDB에서 해당 문서의 데이터만 삭제\n",
    "        try:\n",
    "            if collection:\n",
    "                # 해당 문서의 모든 벡터 데이터 삭제\n",
    "                collection.delete(where={\"document_id\": document_id})\n",
    "                deleted_items.append(f\"벡터 데이터 (document_id: {document_id})\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ChromaDB에서 문서 삭제 중 오류: {e}\")\n",
    "        \n",
    "        logger.info(f\"문서 삭제 완료: {document_id}, 삭제된 항목: {deleted_items}\")\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"message\": f\"문서 '{document_id}' 삭제 완료\",\n",
    "            \"document_id\": document_id,\n",
    "            \"deleted_items\": deleted_items,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"문서 삭제 중 오류: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"문서 삭제 실패: {str(e)}\")\n",
    "\n",
    "@app.delete(\"/delete-all\")\n",
    "async def delete_all_documents():\n",
    "    \"\"\"모든 문서와 데이터를 삭제합니다.\"\"\"\n",
    "    global collection, documents_registry, documents_chunks, documents_bm25\n",
    "    \n",
    "    try:\n",
    "        deleted_items = []\n",
    "        \n",
    "        # 1. 문서 레지스트리 초기화\n",
    "        doc_count = len(documents_registry)\n",
    "        documents_registry.clear()\n",
    "        deleted_items.append(f\"문서 레지스트리 ({doc_count}개 문서)\")\n",
    "        \n",
    "        # 2. 문서별 청크 데이터 초기화\n",
    "        chunk_doc_count = len(documents_chunks)\n",
    "        documents_chunks.clear()\n",
    "        deleted_items.append(f\"문서별 청크 데이터 ({chunk_doc_count}개 문서)\")\n",
    "        \n",
    "        # 3. 문서별 BM25 인덱스 초기화\n",
    "        bm25_doc_count = len(documents_bm25)\n",
    "        documents_bm25.clear()\n",
    "        deleted_items.append(f\"문서별 BM25 인덱스 ({bm25_doc_count}개 문서)\")\n",
    "        \n",
    "        # 4. ChromaDB rag 컬렉션 초기화\n",
    "        try:\n",
    "            if collection:\n",
    "                # 컬렉션의 모든 데이터 삭제\n",
    "                all_data = collection.get()\n",
    "                if all_data['ids']:\n",
    "                    collection.delete(ids=all_data['ids'])\n",
    "                    deleted_items.append(f\"rag 컬렉션 데이터 ({len(all_data['ids'])}개)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ChromaDB 컬렉션 초기화 중 오류: {e}\")\n",
    "        \n",
    "        # 5. 임시 파일들 정리\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_files_deleted = 0\n",
    "        try:\n",
    "            for filename in os.listdir(temp_dir):\n",
    "                if filename.endswith('.pdf') and 'tmp' in filename:\n",
    "                    temp_file_path = os.path.join(temp_dir, filename)\n",
    "                    try:\n",
    "                        os.unlink(temp_file_path)\n",
    "                        temp_files_deleted += 1\n",
    "                    except:\n",
    "                        pass\n",
    "            if temp_files_deleted > 0:\n",
    "                deleted_items.append(f\"임시 PDF 파일들 ({temp_files_deleted}개)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"임시 파일들 정리 중 오류: {e}\")\n",
    "        \n",
    "        logger.info(f\"모든 문서 삭제 완료, 삭제된 항목: {deleted_items}\")\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"message\": \"모든 문서와 데이터 삭제 완료\",\n",
    "            \"deleted_items\": deleted_items,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"전체 삭제 중 오류: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"전체 삭제 실패: {str(e)}\")\n",
    "\n",
    "@app.get(\"/documents\")\n",
    "async def list_documents():\n",
    "    \"\"\"현재 시스템에 등록된 모든 문서 목록을 조회합니다.\"\"\"\n",
    "    try:\n",
    "        # ChromaDB에서 실제 저장된 문서 수 확인\n",
    "        vector_count = 0\n",
    "        unique_documents = set()\n",
    "        \n",
    "        if collection:\n",
    "            try:\n",
    "                all_data = collection.get()\n",
    "                vector_count = len(all_data['ids']) if all_data['ids'] else 0\n",
    "                \n",
    "                # 문서별 벡터 수 계산\n",
    "                for metadata in all_data['metadatas'] or []:\n",
    "                    if 'document_id' in metadata:\n",
    "                        unique_documents.add(metadata['document_id'])\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ChromaDB 데이터 조회 중 오류: {e}\")\n",
    "        \n",
    "        # 문서 레지스트리 정보와 실제 벡터DB 정보 결합\n",
    "        documents_list = []\n",
    "        for doc_id, doc_info in documents_registry.items():\n",
    "            # 실제 벡터DB에서 해당 문서의 벡터 수 확인\n",
    "            actual_vectors = 0\n",
    "            if collection:\n",
    "                try:\n",
    "                    doc_vectors = collection.get(where={\"document_id\": doc_id})\n",
    "                    actual_vectors = len(doc_vectors['ids']) if doc_vectors['ids'] else 0\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            documents_list.append({\n",
    "                **doc_info,\n",
    "                \"actual_vectors_count\": actual_vectors,\n",
    "                \"upload_time_formatted\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(doc_info['upload_time']))\n",
    "            })\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"total_documents\": len(documents_registry),\n",
    "            \"total_vectors\": vector_count,\n",
    "            \"unique_documents_in_db\": len(unique_documents),\n",
    "            \"collection_name\": \"rag\",\n",
    "            \"current_memory_status\": {\n",
    "                \"documents_chunks_loaded\": len(documents_chunks),\n",
    "                \"documents_bm25_loaded\": len(documents_bm25),\n",
    "                \"vector_db_ready\": collection is not None\n",
    "            },\n",
    "            \"documents\": documents_list\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"문서 목록 조회 중 오류: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"문서 목록 조회 실패: {str(e)}\")\n",
    "\n",
    "@app.get(\"/document/{document_id}\")\n",
    "async def get_document_detail(document_id: str):\n",
    "    \"\"\"특정 문서의 상세 정보를 조회합니다.\"\"\"\n",
    "    try:\n",
    "        if document_id not in documents_registry:\n",
    "            raise HTTPException(status_code=404, detail=f\"문서를 찾을 수 없습니다: {document_id}\")\n",
    "        \n",
    "        doc_info = documents_registry[document_id].copy()\n",
    "        \n",
    "        # ChromaDB에서 실제 벡터 데이터 확인\n",
    "        actual_vectors = 0\n",
    "        vector_samples = []\n",
    "        \n",
    "        if collection:\n",
    "            try:\n",
    "                doc_vectors = collection.get(\n",
    "                    where={\"document_id\": document_id},\n",
    "                    limit=5  # 샘플 5개만 가져오기\n",
    "                )\n",
    "                actual_vectors = len(doc_vectors['ids']) if doc_vectors['ids'] else 0\n",
    "                \n",
    "                # 벡터 샘플 정보\n",
    "                if doc_vectors['documents']:\n",
    "                    for i, (doc_text, metadata) in enumerate(zip(doc_vectors['documents'], doc_vectors['metadatas'] or [])):\n",
    "                        vector_samples.append({\n",
    "                            \"sample_id\": i + 1,\n",
    "                            \"text_preview\": doc_text[:150] + \"...\" if len(doc_text) > 150 else doc_text,\n",
    "                            \"parent_section\": metadata.get('parent_section', ''),\n",
    "                            \"page_range\": f\"{metadata.get('page_start', '')}-{metadata.get('page_end', '')}\"\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"문서 벡터 데이터 조회 중 오류: {e}\")\n",
    "        \n",
    "        doc_info.update({\n",
    "            \"actual_vectors_count\": actual_vectors,\n",
    "            \"upload_time_formatted\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(doc_info['upload_time'])),\n",
    "            \"vector_samples\": vector_samples\n",
    "        })\n",
    "        \n",
    "        return JSONResponse(doc_info)\n",
    "    \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"문서 상세 조회 중 오류: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"문서 상세 조회 실패: {str(e)}\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"API 상태 확인\"\"\"\n",
    "    return {\n",
    "        \"message\": \"PDF QA System API\",\n",
    "        \"status\": \"running\",\n",
    "        \"endpoints\": {\n",
    "            \"upload\": \"/upload-pdf\",\n",
    "            \"question\": \"/question\",\n",
    "            \"delete_document\": \"/delete-document/{document_id}\",\n",
    "            \"delete_all\": \"/delete-all\",\n",
    "            \"list_documents\": \"/documents\",\n",
    "            \"document_detail\": \"/document/{document_id}\",\n",
    "            \"status\": \"/status\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/status\")\n",
    "async def get_status():\n",
    "    \"\"\"현재 시스템 상태 확인\"\"\"\n",
    "    total_main_chunks = sum(len(data['main_chunks']) for data in documents_chunks.values())\n",
    "    total_subchunks = sum(len(data['subchunks']) for data in documents_chunks.values())\n",
    "    \n",
    "    return {\n",
    "        \"total_documents\": len(documents_registry),\n",
    "        \"documents_chunks_loaded\": len(documents_chunks),\n",
    "        \"documents_bm25_loaded\": len(documents_bm25),\n",
    "        \"total_main_chunks\": total_main_chunks,\n",
    "        \"total_subchunks\": total_subchunks,\n",
    "        \"vector_db_ready\": collection is not None,\n",
    "        \"documents_info\": [\n",
    "            {\n",
    "                \"document_id\": doc_id,\n",
    "                \"filename\": doc_info[\"filename\"],\n",
    "                \"main_chunks\": len(documents_chunks[doc_id]['main_chunks']) if doc_id in documents_chunks else 0,\n",
    "                \"subchunks\": len(documents_chunks[doc_id]['subchunks']) if doc_id in documents_chunks else 0\n",
    "            }\n",
    "            for doc_id, doc_info in documents_registry.items()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
