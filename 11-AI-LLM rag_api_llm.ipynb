{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6c6113",
   "metadata": {},
   "source": [
    "# PDF RAG ì‹œìŠ¤í…œ ì†ŒìŠ¤ì½”ë“œ ë¶„ì„\n",
    "\n",
    "## ğŸ“‹ ì‹œìŠ¤í…œ ê°œìš”\n",
    "\n",
    "ì´ í”„ë¡œì íŠ¸ëŠ” **PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸-ë‹µë³€ì„ ìˆ˜í–‰í•˜ëŠ” RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œ**ì…ë‹ˆë‹¤. FastAPI ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ë˜ì—ˆìœ¼ë©°, Google Gemini APIë¥¼ í™œìš©í•œ ì„ë² ë”© ìƒì„±ê³¼ ë‹µë³€ ìƒì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ—ï¸ í•µì‹¬ ì•„í‚¤í…ì²˜\n",
    "\n",
    "### **ê³„ì¸µí™”ëœ ì²­í‚¹ ì „ëµ**\n",
    "ì‹œìŠ¤í…œì˜ ê°€ì¥ í˜ì‹ ì ì¸ ë¶€ë¶„ì€ **2ë‹¨ê³„ ì²­í‚¹ êµ¬ì¡°**ì…ë‹ˆë‹¤:\n",
    "\n",
    "```python\n",
    "class MainChunk:\n",
    "    \"\"\"êµ¬ì¡°í™”ëœ ë©”ì¸ ì²­í¬ í´ë˜ìŠ¤\"\"\"\n",
    "    - í˜ì´ì§€ ë²”ìœ„ ê¸°ë°˜ì˜ í° ë‹¨ìœ„ ì²­í¬\n",
    "    - ì„¹ì…˜/ì œëª© ì •ë³´ í¬í•¨\n",
    "    - ì—¬ëŸ¬ ì„œë¸Œì²­í¬ë¥¼ í¬í•¨\n",
    "\n",
    "class SubChunk:\n",
    "    \"\"\"ë²¡í„°í™”ë  ì„œë¸Œì²­í¬ í´ë˜ìŠ¤\"\"\"\n",
    "    - 2-3ë¬¸ì¥ ë‹¨ìœ„ì˜ ì‘ì€ ì²­í¬\n",
    "    - ì‹¤ì œ ë²¡í„° ê²€ìƒ‰ì˜ ëŒ€ìƒ\n",
    "    - ë¶€ëª¨ ì²­í¬ ì°¸ì¡° ì •ë³´ ë³´ìœ \n",
    "```\n",
    "\n",
    "ì´ êµ¬ì¡°ì˜ ì¥ì :\n",
    "- **ì •ë°€í•œ ê²€ìƒ‰**: ì„œë¸Œì²­í¬ë¡œ ì •í™•í•œ ë§¤ì¹­\n",
    "- **í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸**: ë‹µë³€ ìƒì„± ì‹œ ë¶€ëª¨ ì²­í¬ì˜ ì „ì²´ ë‚´ìš© í™œìš©\n",
    "- **ì˜ë¯¸ì  ì¼ê´€ì„±**: ì„¹ì…˜ ë‹¨ìœ„ ì •ë³´ ë³´ì¡´\n",
    "\n",
    "## ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
    "\n",
    "### **ë²¡í„° ê²€ìƒ‰ + BM25 ê²€ìƒ‰ ê²°í•©**\n",
    "\n",
    "```python\n",
    "async def hybrid_search(query: str, document_id: str = None, \n",
    "                       vector_weight: float = 0.7, \n",
    "                       keyword_weight: float = 0.3, \n",
    "                       top_k: int = 5):\n",
    "```\n",
    "\n",
    "**ì ìˆ˜ ê³„ì‚° ë°©ì‹:**\n",
    "```python\n",
    "hybrid_score = (vector_weight * vector_score) + (keyword_weight * bm25_score)\n",
    "```\n",
    "\n",
    "- **ë²¡í„° ê²€ìƒ‰ (70%)**: ì˜ë¯¸ì  ìœ ì‚¬ì„± ê¸°ë°˜\n",
    "- **BM25 ê²€ìƒ‰ (30%)**: í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜\n",
    "- **ê°€ì¤‘ì¹˜ ì¡°ì • ê°€ëŠ¥**: ë„ë©”ì¸ì— ë”°ë¥¸ ìµœì í™” ê°€ëŠ¥\n",
    "\n",
    "## ğŸ“„ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "\n",
    "### **1ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° êµ¬ì¡°í™”**\n",
    "\n",
    "```python\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í˜ì´ì§€ë³„ë¡œ êµ¬ì¡°í™”\"\"\"\n",
    "```\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- PyMuPDFë¥¼ í™œìš©í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "- íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì„¹ì…˜ ì œëª© ìë™ ì¸ì‹\n",
    "- í˜ì´ì§€ë³„ ë©”íƒ€ë°ì´í„° ë³´ì¡´\n",
    "\n",
    "### **2ë‹¨ê³„: ë©”ì¸ ì²­í¬ ìƒì„±**\n",
    "\n",
    "```python\n",
    "def create_main_chunks(pages_data: List[Dict[str, Any]]) -> List[MainChunk]:\n",
    "    \"\"\"í˜ì´ì§€ ë°ì´í„°ë¥¼ í° êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë¶„í• \"\"\"\n",
    "```\n",
    "\n",
    "**ë¡œì§:**\n",
    "- ì„¹ì…˜ ë³€í™” ì§€ì ì—ì„œ ì²­í¬ ë¶„í• \n",
    "- í˜ì´ì§€ ë²”ìœ„ ì •ë³´ ìœ ì§€\n",
    "- ìµœì†Œ ì²­í¬ ìˆ˜ ë³´ì¥ (í˜ì´ì§€ ë‹¨ìœ„ í´ë°±)\n",
    "\n",
    "### **3ë‹¨ê³„: ì„œë¸Œì²­í¬ ìƒì„±**\n",
    "\n",
    "```python\n",
    "def create_subchunks_from_main_chunk(main_chunk: MainChunk, \n",
    "                                   target_tokens: int = 150) -> List[SubChunk]:\n",
    "    \"\"\"ë©”ì¸ ì²­í¬ì—ì„œ 2-3ë¬¸ì¥ ë‹¨ìœ„ì˜ ì„œë¸Œì²­í¬ ìƒì„±\"\"\"\n",
    "```\n",
    "\n",
    "**ìµœì í™”ëœ ë¶„í•  ê¸°ì¤€:**\n",
    "- ëª©í‘œ í† í° ìˆ˜: 150ê°œ\n",
    "- ë¬¸ì¥ ìˆ˜: 2-3ë¬¸ì¥\n",
    "- í† í° ì˜¤ë²„í”Œë¡œìš° ë°©ì§€\n",
    "- ë¬¸ë§¥ ê²½ê³„ ë³´ì¡´\n",
    "\n",
    "## ğŸ¤– AI ëª¨ë¸ í†µí•©\n",
    "\n",
    "### **Gemini API í™œìš©**\n",
    "\n",
    "```python\n",
    "# ì„ë² ë”© ìƒì„±\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "# ë‹µë³€ ìƒì„±\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "```\n",
    "\n",
    "**ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”:**\n",
    "- 30ê°œ ë‹¨ìœ„ ë°°ì¹˜ ì„ë² ë”© ìƒì„±\n",
    "- API ì œí•œ ê³ ë ¤í•œ ì²˜ë¦¬ëŸ‰ ì¡°ì ˆ\n",
    "- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
    "\n",
    "## ğŸ’¾ ë°ì´í„° ì €ì¥ ë° ê´€ë¦¬\n",
    "\n",
    "### **ChromaDB ë²¡í„° ì €ì¥ì†Œ**\n",
    "\n",
    "```python\n",
    "collection.add(\n",
    "    documents=batch_texts,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadata_list,\n",
    "    ids=[f\"{document_id}_{subchunk.subchunk_id}\" for subchunk in batch_subchunks]\n",
    ")\n",
    "```\n",
    "\n",
    "**ë©”íƒ€ë°ì´í„° êµ¬ì¡°:**\n",
    "- `document_id`: ë¬¸ì„œ ê³ ìœ  ì‹ë³„ì\n",
    "- `parent_chunk_id`: ë¶€ëª¨ ì²­í¬ ì°¸ì¡°\n",
    "- `page_start/end`: í˜ì´ì§€ ë²”ìœ„\n",
    "- `section/title`: êµ¬ì¡°ì  ì •ë³´\n",
    "\n",
    "### **ë©”ëª¨ë¦¬ ê¸°ë°˜ ë³´ì¡° ì €ì¥ì†Œ**\n",
    "\n",
    "```python\n",
    "documents_registry = {}    # ë¬¸ì„œ ë©”íƒ€ì •ë³´\n",
    "documents_chunks = {}      # ì²­í¬ ë°ì´í„°\n",
    "documents_bm25 = {}       # BM25 ì¸ë±ìŠ¤\n",
    "```\n",
    "\n",
    "## ğŸ¯ ì§ˆë¬¸-ë‹µë³€ í”„ë¡œì„¸ìŠ¤\n",
    "\n",
    "### **ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì „ëµ**\n",
    "\n",
    "```python\n",
    "# ë¶€ëª¨ ì²­í¬ì˜ ì „ì²´ í…ìŠ¤íŠ¸ í™œìš©\n",
    "context_part = f\"[{info['title']} - í˜ì´ì§€ {info['page_start']}\"\n",
    "context_part += f\", ì„¹ì…˜: {info['section']}]\\n{info['parent_text']}\"\n",
    "```\n",
    "\n",
    "**í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§:**\n",
    "- ëª…í™•í•œ ì—­í•  ì •ì˜\n",
    "- ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ê°•ì œ\n",
    "- í˜ì´ì§€/ì„¹ì…˜ ì •ë³´ í¬í•¨ ì§€ì‹œ\n",
    "- í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ ìš”êµ¬\n",
    "\n",
    "## ğŸ”§ API ì—”ë“œí¬ì¸íŠ¸\n",
    "\n",
    "### **í•µì‹¬ ê¸°ëŠ¥**\n",
    "\n",
    "| ì—”ë“œí¬ì¸íŠ¸ | ê¸°ëŠ¥ | ì„¤ëª… |\n",
    "|-----------|------|------|\n",
    "| `POST /upload-pdf` | ë¬¸ì„œ ì—…ë¡œë“œ | PDF ì²˜ë¦¬ ë° ë²¡í„°í™” |\n",
    "| `POST /question` | ì§ˆë¬¸ ë‹µë³€ | í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + LLM ìƒì„± |\n",
    "| `GET /documents` | ë¬¸ì„œ ëª©ë¡ | ë“±ë¡ëœ ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ |\n",
    "| `DELETE /delete-document/{id}` | ë¬¸ì„œ ì‚­ì œ | íŠ¹ì • ë¬¸ì„œ ì™„ì „ ì‚­ì œ |\n",
    "\n",
    "### **ê´€ë¦¬ ê¸°ëŠ¥**\n",
    "\n",
    "```python\n",
    "@app.get(\"/status\")\n",
    "async def get_status():\n",
    "    \"\"\"ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§\"\"\"\n",
    "    \n",
    "@app.delete(\"/delete-all\") \n",
    "async def delete_all_documents():\n",
    "    \"\"\"ì „ì²´ ë°ì´í„° ì´ˆê¸°í™”\"\"\"\n",
    "```\n",
    "\n",
    "## âš¡ ì„±ëŠ¥ ìµœì í™” ìš”ì†Œ\n",
    "\n",
    "### **ë°°ì¹˜ ì²˜ë¦¬**\n",
    "- ì„ë² ë”© ìƒì„±: 30ê°œ ë‹¨ìœ„ ë°°ì¹˜\n",
    "- API í˜¸ì¶œ ìµœì†Œí™”\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤\n",
    "\n",
    "### **ë¹„ë™ê¸° ì²˜ë¦¬**\n",
    "```python\n",
    "async def create_embeddings_batch(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"ë¹„ë™ê¸° ì„ë² ë”© ìƒì„±\"\"\"\n",
    "```\n",
    "\n",
    "### **ì¸ë±ìŠ¤ ìµœì í™”**\n",
    "- BM25 ì‚¬ì „ ê³„ì‚° ë° ìºì‹±\n",
    "- ChromaDB ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸ë±ìŠ¤\n",
    "- ë¬¸ì„œë³„ ê²©ë¦¬ëœ ê²€ìƒ‰ ì§€ì›\n",
    "\n",
    "## ğŸ› ï¸ ì£¼ìš” ì˜ì¡´ì„±\n",
    "\n",
    "```python\n",
    "# í•µì‹¬ í”„ë ˆì„ì›Œí¬\n",
    "fastapi              # ì›¹ API í”„ë ˆì„ì›Œí¬\n",
    "pydantic            # ë°ì´í„° ê²€ì¦\n",
    "\n",
    "# PDF ì²˜ë¦¬\n",
    "pymupdf             # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "\n",
    "# AI/ML\n",
    "langchain_google_genai  # Gemini API ë˜í¼\n",
    "tiktoken            # í† í° ì¹´ìš´íŒ…\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ\n",
    "chromadb            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤\n",
    "\n",
    "# ê²€ìƒ‰ ë° NLP\n",
    "rank_bm25           # BM25 ì•Œê³ ë¦¬ì¦˜\n",
    "sklearn             # TF-IDF ë²¡í„°í™”\n",
    "```\n",
    "\n",
    "## ğŸ’¡ ì‹œìŠ¤í…œì˜ ê°•ì \n",
    "\n",
    "1. **ê³„ì¸µí™”ëœ ì²­í‚¹**: ì •ë°€ ê²€ìƒ‰ + í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸\n",
    "2. **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰**: ì˜ë¯¸ì  + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©\n",
    "3. **êµ¬ì¡° ë³´ì¡´**: PDFì˜ ì„¹ì…˜/í˜ì´ì§€ ì •ë³´ í™œìš©\n",
    "4. **í™•ì¥ì„±**: ë©€í‹° ë¬¸ì„œ ì§€ì› ë° ê°œë³„ ê´€ë¦¬\n",
    "5. **ìµœì í™”**: ë°°ì¹˜ ì²˜ë¦¬ ë° ë¹„ë™ê¸° API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042776f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc242f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
    "from fastapi.responses import JSONResponse\n",
    "from pydantic import BaseModel\n",
    "import pymupdf\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import tiktoken\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "import uuid\n",
    "import tempfile\n",
    "import os\n",
    "import chromadb\n",
    "from rank_bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import logging\n",
    "import time\n",
    "import shutil\n",
    "from dotenv import load_dotenv\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "\n",
    "# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ\n",
    "load_dotenv()\n",
    "os.environ[\"CHROMA_TELEMETRY_ANONYMOUS\"] = \"False\"\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", google_api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# chromadb telemetry ë¡œê·¸ ë ˆë²¨ WARNING ì´ìƒìœ¼ë¡œ ê²©ìƒ(ìˆ¨ê¹€)\n",
    "logging.getLogger(\"chromadb.telemetry\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"chromadb.telemetry.product.posthog\").setLevel(logging.CRITICAL)\n",
    "\n",
    "app = FastAPI(title=\"PDF QA System (Gemini)\", version=\"1.0.0\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# ì „ì—­ ë³€ìˆ˜ë“¤\n",
    "collection = None\n",
    "documents_registry = {}  # ë¬¸ì„œ ì •ë³´ë¥¼ ì €ì¥í•˜ëŠ” ë ˆì§€ìŠ¤íŠ¸ë¦¬\n",
    "documents_chunks = {}  # ë¬¸ì„œë³„ ì²­í¬ ë°ì´í„° ì €ì¥\n",
    "documents_bm25 = {}  # ë¬¸ì„œë³„ BM25 ì¸ë±ìŠ¤ ì €ì¥\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 í† í¬ë‚˜ì´ì €(ì„ë² ë”© í† í° ì¹´ìš´íŠ¸ìš©)\n",
    "\n",
    "class QuestionRequest(BaseModel):\n",
    "    question: str\n",
    "    document_id: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3861a20",
   "metadata": {},
   "source": [
    "# ğŸ“š 2ë‹¨ê³„ ì²­í‚¹ ì „ëµ: ì±…ìœ¼ë¡œ ì´í•´í•˜ê¸°\n",
    "\n",
    "## ğŸ¤” ì™œ ì±…ì„ ì±•í„°ì™€ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì½ì„ê¹Œìš”?\n",
    "\n",
    "ì±…ì„ ì½ì„ ë•Œ ìš°ë¦¬ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ **2ë‹¨ê³„ë¡œ ì •ë³´ë¥¼ ì²˜ë¦¬**í•©ë‹ˆë‹¤:\n",
    "\n",
    "### ğŸ“– ì¼ë°˜ì ì¸ ë…ì„œ ê³¼ì •\n",
    "```\n",
    "1ë‹¨ê³„: ëª©ì°¨ë¥¼ ë³´ê³  ì›í•˜ëŠ” ì±•í„° ì°¾ê¸°\n",
    "       \"ì•„, ë§ˆì¼€íŒ… ì „ëµì€ 3ì±•í„°ì— ìˆêµ¬ë‚˜!\"\n",
    "\n",
    "2ë‹¨ê³„: ì±•í„° ì•ˆì—ì„œ êµ¬ì²´ì ì¸ ë¬¸ë‹¨ ì°¾ê¸°  \n",
    "       \"ëª©í‘œ ê³ ê°ì— ëŒ€í•œ ë‚´ìš©ì€ ì´ ë¬¸ë‹¨ì´ë„¤!\"\n",
    "\n",
    "3ë‹¨ê³„: ì „ì²´ ì±•í„°ë¥¼ ì½ìœ¼ë©° ë§¥ë½ ì´í•´\n",
    "       \"ì•ë’¤ ë‚´ìš©ê¹Œì§€ ì½ì–´ì•¼ ì™„ì „íˆ ì´í•´ë˜ê² ë‹¤\"\n",
    "```\n",
    "\n",
    "RAG ì‹œìŠ¤í…œë„ ë˜‘ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š MainChunk = ì±…ì˜ ì±•í„°(Chapter)\n",
    "\n",
    "### ğŸ“– ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì„œì  ì˜ˆì‹œ\n",
    "\n",
    "```python\n",
    "class MainChunk:\n",
    "    \"\"\"ì±…ì˜ í•œ ì±•í„°\"\"\"\n",
    "    def __init__(self, text: str, chunk_id: str, page_start: int, page_end: int, \n",
    "                 section: str = \"\", title: str = \"\"):\n",
    "        self.text = text          # ğŸ“„ ì±•í„° ì „ì²´ ë‚´ìš©\n",
    "        self.chunk_id = chunk_id  # ğŸ·ï¸ ì±•í„° ê³ ìœ ë²ˆí˜¸  \n",
    "        self.page_start = 15      # ğŸ“– ì±•í„° ì‹œì‘ í˜ì´ì§€\n",
    "        self.page_end = 28        # ğŸ“– ì±•í„° ë í˜ì´ì§€  \n",
    "        self.section = \"ì œ3ì¥\"    # ğŸ“‚ ì¥ ë²ˆí˜¸\n",
    "        self.title = \"ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ\"  # ğŸ“ ì±•í„° ì œëª©\n",
    "        self.subchunks = []       # ğŸ“ ì´ ì±•í„°ì˜ ëª¨ë“  ë¬¸ë‹¨ë“¤\n",
    "```\n",
    "\n",
    "### ğŸ“š ì‹¤ì œ ì±•í„° ë‚´ìš© ì˜ˆì‹œ\n",
    "```\n",
    "ğŸ“– \"ì„±ê³µí•˜ëŠ” ìŠ¤íƒ€íŠ¸ì—…ì˜ ë§ˆì¼€íŒ…\" ì±…ì˜ ì œ3ì¥\n",
    "\n",
    "ì œ3ì¥: ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ (15-28í˜ì´ì§€)\n",
    "â”œâ”€â”€ 3.1 ì†Œì…œë¯¸ë””ì–´ ë§ˆì¼€íŒ…\n",
    "â”œâ”€â”€ 3.2 ì½˜í…ì¸  ë§ˆì¼€íŒ… ì „ëµ  \n",
    "â”œâ”€â”€ 3.3 ì¸í”Œë£¨ì–¸ì„œ í˜‘ì—…\n",
    "â”œâ”€â”€ 3.4 ê´‘ê³  ì˜ˆì‚° ë°°ë¶„\n",
    "â””â”€â”€ 3.5 ì„±ê³¼ ì¸¡ì • ë°©ë²•\n",
    "\n",
    "ì „ì²´ í…ìŠ¤íŠ¸: \"ë””ì§€í„¸ ì‹œëŒ€ì— ë§ˆì¼€íŒ…ì€ ê¸°ì—…ì˜ ìƒì¡´ì„ ì¢Œìš°í•©ë‹ˆë‹¤. \n",
    "ì†Œì…œë¯¸ë””ì–´ëŠ” ê³ ê°ê³¼ì˜ ì§ì ‘ì ì¸ ì†Œí†µ ì°½êµ¬ì…ë‹ˆë‹¤... \n",
    "(14í˜ì´ì§€ ë¶„ëŸ‰ì˜ ìƒì„¸í•œ ë‚´ìš©)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ SubChunk = ì±…ì˜ ë¬¸ë‹¨(Paragraph)\n",
    "\n",
    "### âœï¸ ì‹¤ì œ ë¬¸ë‹¨ ì˜ˆì‹œ\n",
    "\n",
    "```python\n",
    "class SubChunk:\n",
    "    \"\"\"ì±…ì˜ í•œ ë¬¸ë‹¨\"\"\"\n",
    "    def __init__(self, text: str, subchunk_id: str, parent_chunk_id: str, \n",
    "                 sentence_start: int, sentence_end: int):\n",
    "        self.text = text                 # ğŸ“ ë¬¸ë‹¨ ë‚´ìš© (2-3ë¬¸ì¥)\n",
    "        self.subchunk_id = subchunk_id   # ğŸ·ï¸ ë¬¸ë‹¨ ê³ ìœ ë²ˆí˜¸\n",
    "        self.parent_chunk_id = parent_chunk_id  # ğŸ“š ì–´ëŠ ì±•í„°ì— ì†í•˜ëŠ”ì§€\n",
    "        self.sentence_start = sentence_start    # ğŸ“ ë¬¸ë‹¨ì˜ ì‹œì‘ ë¬¸ì¥ ë²ˆí˜¸\n",
    "        self.sentence_end = sentence_end        # ğŸ“ ë¬¸ë‹¨ì˜ ë ë¬¸ì¥ ë²ˆí˜¸\n",
    "```\n",
    "\n",
    "### ğŸ“„ ì‹¤ì œ ë¬¸ë‹¨ë“¤ ì˜ˆì‹œ\n",
    "\n",
    "**SubChunk 1: ì¸ìŠ¤íƒ€ê·¸ë¨ ë§ˆì¼€íŒ…**\n",
    "```python\n",
    "subchunk_1 = SubChunk(\n",
    "    text=\"ì¸ìŠ¤íƒ€ê·¸ë¨ì€ 2030 ì—¬ì„± ê³ ê°ì¸µì—ê²Œ ê°€ì¥ íš¨ê³¼ì ì¸ í”Œë«í¼ì…ë‹ˆë‹¤. \n",
    "          ì‹œê°ì  ì½˜í…ì¸ ë¥¼ í†µí•´ ë¸Œëœë“œ ìŠ¤í† ë¦¬ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n",
    "          í•´ì‹œíƒœê·¸ ì „ëµì´ ë…¸ì¶œë„ë¥¼ í¬ê²Œ ì¢Œìš°í•©ë‹ˆë‹¤.\",\n",
    "    subchunk_id=\"sub_3_1_1\",\n",
    "    parent_chunk_id=\"chapter_3\",  # ì œ3ì¥ì— ì†í•¨\n",
    "    sentence_start=1,\n",
    "    sentence_end=3\n",
    ")\n",
    "```\n",
    "\n",
    "**SubChunk 2: ìœ íŠœë¸Œ ë§ˆì¼€íŒ…**\n",
    "```python\n",
    "subchunk_2 = SubChunk(\n",
    "    text=\"ìœ íŠœë¸ŒëŠ” ëª¨ë“  ì—°ë ¹ëŒ€ì—ì„œ ê°•ë ¥í•œ ì˜í–¥ë ¥ì„ ë³´ì…ë‹ˆë‹¤. \n",
    "          êµìœ¡ì  ì½˜í…ì¸ ì™€ ì—”í„°í…Œì¸ë¨¼íŠ¸ë¥¼ ê²°í•©í•˜ë©´ íš¨ê³¼ê°€ ê·¹ëŒ€í™”ë©ë‹ˆë‹¤.\",\n",
    "    subchunk_id=\"sub_3_1_2\", \n",
    "    parent_chunk_id=\"chapter_3\",  # ê°™ì€ ì œ3ì¥ì— ì†í•¨\n",
    "    sentence_start=4,\n",
    "    sentence_end=5\n",
    ")\n",
    "```\n",
    "\n",
    "**SubChunk 3: ì½˜í…ì¸  ê¸°íš**\n",
    "```python\n",
    "subchunk_3 = SubChunk(\n",
    "    text=\"ì½˜í…ì¸  ê¸°íš ì‹œ ê³ ê° í˜ë¥´ì†Œë‚˜ë¥¼ ëª…í™•íˆ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "          ê°ì •ì  ì—°ê²°ê³ ë¦¬ê°€ ìˆëŠ” ìŠ¤í† ë¦¬í…”ë§ì´ í•µì‹¬ì…ë‹ˆë‹¤.\",\n",
    "    subchunk_id=\"sub_3_2_1\",\n",
    "    parent_chunk_id=\"chapter_3\",  # ê°™ì€ ì œ3ì¥ì— ì†í•¨  \n",
    "    sentence_start=15,\n",
    "    sentence_end=16\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” ì‹¤ì œ ë™ì‘ ê³¼ì •: ë…ì„œí•˜ëŠ” AI\n",
    "\n",
    "### ğŸ™‹â€â™€ï¸ ì‚¬ìš©ì ì§ˆë¬¸\n",
    "```\n",
    "\"ì¸ìŠ¤íƒ€ê·¸ë¨ ë§ˆì¼€íŒ…ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "```\n",
    "\n",
    "### ğŸ” 1ë‹¨ê³„: ì±… ì „ì²´ì—ì„œ ê´€ë ¨ ë¬¸ë‹¨ ì°¾ê¸° (SubChunk ê²€ìƒ‰)\n",
    "\n",
    "**AIê°€ ëª¨ë“  ë¬¸ë‹¨ì„ ë¹ ë¥´ê²Œ í›‘ì–´ë´…ë‹ˆë‹¤**\n",
    "```python\n",
    "ê²€ìƒ‰ ê²°ê³¼:\n",
    "1. subchunk_3_1_1: \"ì¸ìŠ¤íƒ€ê·¸ë¨ì€ 2030 ì—¬ì„± ê³ ê°ì¸µì—ê²Œ...\" (ê´€ë ¨ë„: 95%)\n",
    "2. subchunk_5_2_3: \"ì¸ìŠ¤íƒ€ê·¸ë¨ ê´‘ê³ ë¹„ ì±…ì • ë°©ë²•...\" (ê´€ë ¨ë„: 78%)  \n",
    "3. subchunk_7_1_2: \"ì¸ìŠ¤íƒ€ê·¸ë¨ ë¶„ì„ íˆ´ ì‚¬ìš©ë²•...\" (ê´€ë ¨ë„: 65%)\n",
    "```\n",
    "\n",
    "### ğŸ“– 2ë‹¨ê³„: í•´ë‹¹ ì±•í„° ì „ì²´ ì½ê¸° (MainChunk í™œìš©)\n",
    "\n",
    "**ê°€ì¥ ê´€ë ¨ë„ ë†’ì€ ë¬¸ë‹¨ì´ ì†í•œ ì±•í„°ë¥¼ ì „ë¶€ ì½ìŠµë‹ˆë‹¤**\n",
    "```python\n",
    "subchunk_3_1_1 â†’ parent_chunk_id: \"chapter_3\"\n",
    "â†’ ì œ3ì¥ \"ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ\" ì „ì²´ ë‚´ìš© (15-28í˜ì´ì§€) í™œìš©\n",
    "```\n",
    "\n",
    "### ğŸ“ 3ë‹¨ê³„: ë§¥ë½ì„ ì´í•´í•œ ì™„ì „í•œ ë‹µë³€\n",
    "\n",
    "**AIê°€ ë‹µë³€í•  ë•Œ:**\n",
    "```\n",
    "âœ… ì¢‹ì€ ë‹µë³€:\n",
    "\"ì¸ìŠ¤íƒ€ê·¸ë¨ ë§ˆì¼€íŒ…ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ í•´ì‹œíƒœê·¸ ì „ëµì…ë‹ˆë‹¤. \n",
    "(ì œ3ì¥ ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ, 17í˜ì´ì§€ ì°¸ì¡°)\n",
    "\n",
    "íŠ¹íˆ 2030 ì—¬ì„± ê³ ê°ì¸µì„ íƒ€ê²Ÿìœ¼ë¡œ í•  ë•Œ íš¨ê³¼ì ì´ë©°, \n",
    "ì‹œê°ì  ì½˜í…ì¸ ë¥¼ í†µí•œ ë¸Œëœë“œ ìŠ¤í† ë¦¬í…”ë§ê³¼ í•¨ê»˜ \n",
    "í•´ì‹œíƒœê·¸ ì „ëµì„ êµ¬ì„±í•˜ë©´ ë…¸ì¶œë„ë¥¼ í¬ê²Œ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ë˜í•œ ê°™ì€ ì¥ì—ì„œ ì–¸ê¸‰í•˜ë“¯ì´, ì½˜í…ì¸  ê¸°íš ì‹œ \n",
    "ê³ ê° í˜ë¥´ì†Œë‚˜ë¥¼ ëª…í™•íˆ ì„¤ì •í•˜ê³  ê°ì •ì  ì—°ê²°ê³ ë¦¬ê°€ ìˆëŠ” \n",
    "ìŠ¤í† ë¦¬í…”ë§ì„ ê²°í•©í•˜ë©´ ë”ìš± íš¨ê³¼ì ì…ë‹ˆë‹¤.\"\n",
    "\n",
    "ì¶œì²˜: ì œ3ì¥ ë””ì§€í„¸ ë§ˆì¼€íŒ… ì „ëµ (15-28í˜ì´ì§€)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š ë‹¤ë¥¸ ì˜ˆì‹œ: IT ê¸°ìˆ ì„œì \n",
    "\n",
    "### ğŸ“– \"íŒŒì´ì¬ ì›¹ê°œë°œ ì™„ë²½ê°€ì´ë“œ\" ì±…\n",
    "\n",
    "**MainChunk: ì œ7ì¥ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™**\n",
    "```python\n",
    "main_chunk = MainChunk(\n",
    "    text=\"ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í•µì‹¬ì…ë‹ˆë‹¤...(20í˜ì´ì§€ ë¶„ëŸ‰)\",\n",
    "    chunk_id=\"chapter_7\", \n",
    "    page_start=89,\n",
    "    page_end=109,\n",
    "    section=\"ì œ7ì¥\",\n",
    "    title=\"ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™\",\n",
    "    subchunks=[sub_7_1, sub_7_2, sub_7_3, ...]\n",
    ")\n",
    "```\n",
    "\n",
    "**SubChunkë“¤**\n",
    "```python\n",
    "# SQLAlchemy ì„¤ì¹˜ ë°©ë²•\n",
    "sub_7_1 = SubChunk(\n",
    "    text=\"SQLAlchemyëŠ” pip install sqlalchemyë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤. \n",
    "          ê°€ìƒí™˜ê²½ ì‚¬ìš©ì„ ê°•ë ¥íˆ ê¶Œì¥í•©ë‹ˆë‹¤.\",\n",
    "    subchunk_id=\"sub_7_1\",\n",
    "    parent_chunk_id=\"chapter_7\"\n",
    ")\n",
    "\n",
    "# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •  \n",
    "sub_7_2 = SubChunk(\n",
    "    text=\"ë°ì´í„°ë² ì´ìŠ¤ URL í˜•ì‹ì€ 'dialect://user:password@host/database'ì…ë‹ˆë‹¤. \n",
    "          í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³´ì•ˆì„ ê°•í™”í•˜ì„¸ìš”.\",\n",
    "    subchunk_id=\"sub_7_2\", \n",
    "    parent_chunk_id=\"chapter_7\"\n",
    ")\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜ ë°©ë²•\n",
    "sub_7_3 = SubChunk(\n",
    "    text=\"SQLAlchemy ëª¨ë¸ì€ db.Modelì„ ìƒì†ë°›ì•„ ì •ì˜í•©ë‹ˆë‹¤. \n",
    "          ê° ì»¬ëŸ¼ì€ db.Columnìœ¼ë¡œ ì„ ì–¸í•˜ë©° íƒ€ì…ì„ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\",\n",
    "    subchunk_id=\"sub_7_3\",\n",
    "    parent_chunk_id=\"chapter_7\"  \n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸ¤– ì§ˆë¬¸ê³¼ ë‹µë³€ ì˜ˆì‹œ\n",
    "\n",
    "**ì§ˆë¬¸:** \"SQLAlchemy ëª¨ë¸ì€ ì–´ë–»ê²Œ ì •ì˜í•˜ë‚˜ìš”?\"\n",
    "\n",
    "**ê²€ìƒ‰ ê³¼ì •:**\n",
    "```\n",
    "1. SubChunk ê²€ìƒ‰: sub_7_3ì´ ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ (92% ë§¤ì¹˜)\n",
    "2. MainChunk í™•ì¥: chapter_7 ì „ì²´ ë‚´ìš© í™œìš©\n",
    "3. ë§¥ë½ ìˆëŠ” ë‹µë³€ ìƒì„±\n",
    "```\n",
    "\n",
    "**ë‹µë³€:**\n",
    "```\n",
    "SQLAlchemy ëª¨ë¸ì€ db.Modelì„ ìƒì†ë°›ì•„ ì •ì˜í•©ë‹ˆë‹¤. \n",
    "ê° ì»¬ëŸ¼ì€ db.Columnìœ¼ë¡œ ì„ ì–¸í•˜ë©° íƒ€ì…ì„ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´:\n",
    "class User(db.Model):\n",
    "    id = db.Column(db.Integer, primary_key=True)\n",
    "    username = db.Column(db.String(80), unique=True)\n",
    "\n",
    "ëª¨ë¸ ì •ì˜ ì „ì— SQLAlchemyë¥¼ ì„¤ì¹˜í•˜ê³  (pip install sqlalchemy), \n",
    "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "ìì„¸í•œ ì„¤ì¹˜ ë° ì„¤ì • ë°©ë²•ì€ ê°™ì€ ì¥ì˜ ì•ë¶€ë¶„ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n",
    "\n",
    "ì¶œì²˜: ì œ7ì¥ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (89-109í˜ì´ì§€)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ ì™œ ì´ ë°©ì‹ì´ íš¨ê³¼ì ì¸ê°€?\n",
    "\n",
    "### ğŸ¯ ì •í™•í•œ ê²€ìƒ‰ = ì±…ì˜ ìƒ‰ì¸ í™œìš©\n",
    "```python\n",
    "# ë‚˜ìœ ì˜ˆ: ì±… ì „ì²´ì—ì„œ ì°¾ê¸°\n",
    "\"íŒŒì´ì¬ ì±… 500í˜ì´ì§€ ì „ì²´ì—ì„œ SQLAlchemy ê´€ë ¨ ë‚´ìš© ì°¾ì•„ì¤˜\"\n",
    "â†’ ë„ˆë¬´ ê´‘ë²”ìœ„í•˜ê³  ë¶€ì •í™•\n",
    "\n",
    "# ì¢‹ì€ ì˜ˆ: ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ì •í™•íˆ ì°¾ê¸°  \n",
    "\"SQLAlchemy ëª¨ë¸ ì •ì˜\" ë¬¸ë‹¨ì„ ì •í™•íˆ ë°œê²¬\n",
    "â†’ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ì •ë³´ íšë“\n",
    "```\n",
    "\n",
    "### ğŸ“– í’ë¶€í•œ ë§¥ë½ = ì±•í„° ì „ì²´ ì½ê¸°\n",
    "```python\n",
    "# ë‹¨í¸ì  ì •ë³´ë§Œ ì œê³µí•˜ëŠ” ê²½ìš°\n",
    "\"db.Modelì„ ìƒì†ë°›ì•„ ì •ì˜í•©ë‹ˆë‹¤.\"\n",
    "â†’ ì„¤ì¹˜ ë°©ë²•, ì„¤ì • ë°©ë²•ì„ ëª¨ë¦„\n",
    "\n",
    "# ì±•í„° ì „ì²´ ë§¥ë½ì„ í™œìš©í•˜ëŠ” ê²½ìš°\n",
    "\"ì„¤ì¹˜ â†’ ì„¤ì • â†’ ëª¨ë¸ ì •ì˜ â†’ ì‚¬ìš©ë²•\"ì˜ ì „ì²´ í”Œë¡œìš° ì œê³µ\n",
    "â†’ ì™„ì „í•˜ê³  ì‹¤ìš©ì ì¸ ë‹µë³€\n",
    "```\n",
    "\n",
    "### ğŸ“ ì¶œì²˜ ëª…í™•ì„± = í˜ì´ì§€ ë²ˆí˜¸ ì œê³µ\n",
    "```python\n",
    "return {\n",
    "    \"answer\": \"SQLAlchemy ëª¨ë¸ì€...\",\n",
    "    \"source\": \"ì œ7ì¥ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ (89-109í˜ì´ì§€)\",\n",
    "    \"specific_section\": \"7.3 ëª¨ë¸ ì •ì˜\"\n",
    "}\n",
    "â†’ ì‚¬ìš©ìê°€ ì›ë³¸ ì±…ì—ì„œ ì¶”ê°€ í™•ì¸ ê°€ëŠ¥\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ í•µì‹¬ ì •ë¦¬\n",
    "\n",
    "### ğŸ“š MainChunk (ì±•í„°) = ìˆ²ì„ ë³´ëŠ” ê´€ì \n",
    "- **ì—­í• **: ì „ì²´ì ì¸ ë§¥ë½ê³¼ êµ¬ì¡° ì œê³µ\n",
    "- **íŠ¹ì§•**: í•œ ì£¼ì œì— ëŒ€í•œ ì™„ì „í•œ ì„¤ëª…\n",
    "- **í™œìš©**: ë‹µë³€ ìƒì„± ì‹œ í’ë¶€í•œ ë°°ê²½ì§€ì‹ ì œê³µ\n",
    "\n",
    "### ğŸ“ SubChunk (ë¬¸ë‹¨) = ë‚˜ë¬´ë¥¼ ë³´ëŠ” ê´€ì   \n",
    "- **ì—­í• **: ì •í™•í•œ ì •ë³´ ê²€ìƒ‰\n",
    "- **íŠ¹ì§•**: 2-3ë¬¸ì¥ì˜ êµ¬ì²´ì  ë‚´ìš©\n",
    "- **í™œìš©**: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ì •ë°€í•œ ë§¤ì¹­\n",
    "\n",
    "### ğŸ¤ í˜‘ë ¥ íš¨ê³¼ = ì™„ë²½í•œ ë…ì„œ\n",
    "- **ê²€ìƒ‰**: SubChunkë¡œ ì •í™•í•œ ë¬¸ë‹¨ ì°¾ê¸°\n",
    "- **ì´í•´**: MainChunkë¡œ ì „ì²´ ë§¥ë½ íŒŒì•…  \n",
    "- **ë‹µë³€**: ì •í™•í•˜ë©´ì„œë„ ì™„ì „í•œ ì •ë³´ ì œê³µ\n",
    "\n",
    "**ê²°ê³¼ì ìœ¼ë¡œ \"ì •í™•í•˜ë©´ì„œë„ ì™„ì „í•œ\" ë‹µë³€ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤!** ğŸ“–âœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356b691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MainChunk:\n",
    "    \"\"\"êµ¬ì¡°í™”ëœ ë©”ì¸ ì²­í¬ í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self, text: str, chunk_id: str, page_start: int, page_end: int, section: str = \"\", title: str = \"\"):\n",
    "        self.text = text\n",
    "        self.chunk_id = chunk_id\n",
    "        self.page_start = page_start\n",
    "        self.page_end = page_end\n",
    "        self.section = section\n",
    "        self.title = title\n",
    "        self.subchunks = []  # ì´ ì²­í¬ì— ì†í•œ ì„œë¸Œì²­í¬ë“¤\n",
    "\n",
    "class SubChunk:\n",
    "    \"\"\"ë²¡í„°í™”ë  ì„œë¸Œì²­í¬ í´ë˜ìŠ¤\"\"\"\n",
    "    def __init__(self, text: str, subchunk_id: str, parent_chunk_id: str, sentence_start: int, sentence_end: int):\n",
    "        self.text = text\n",
    "        self.subchunk_id = subchunk_id\n",
    "        self.parent_chunk_id = parent_chunk_id\n",
    "        self.sentence_start = sentence_start\n",
    "        self.sentence_end = sentence_end\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³  í˜ì´ì§€ë³„ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.\"\"\"\n",
    "    doc = pymupdf.open(pdf_path)\n",
    "    pages_data = []\n",
    "    \n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # ëª©ì°¨ë‚˜ ì„¹ì…˜ ì œëª© ì¶”ì¶œ (ê°œì„ ëœ íœ´ë¦¬ìŠ¤í‹±)\n",
    "        lines = text.split('\\n')\n",
    "        section_title = \"\"\n",
    "        \n",
    "        # ì œëª© íŒ¨í„´ ì°¾ê¸° (ìˆ«ì. ì œëª©, ëŒ€ë¬¸ì ì œëª©, ë“±)\n",
    "        for line in lines[:10]:  # ìƒìœ„ 10ì¤„ í™•ì¸\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì œëª© (1. ì œëª©, 1.1 ì œëª© ë“±)\n",
    "                if re.match(r'^\\d+\\.?\\d*\\.?\\s+[A-Za-zê°€-í£]', line):\n",
    "                    section_title = line\n",
    "                    break\n",
    "                # ì „ì²´ ëŒ€ë¬¸ì ì œëª©\n",
    "                elif line.isupper() and 5 <= len(line) <= 50:\n",
    "                    section_title = line\n",
    "                    break\n",
    "                # ì²« ê¸€ìë§Œ ëŒ€ë¬¸ìì´ê³  ì ì ˆí•œ ê¸¸ì´\n",
    "                elif line[0].isupper() and 10 <= len(line) <= 80 and line.count(' ') <= 8:\n",
    "                    section_title = line\n",
    "                    break\n",
    "        \n",
    "        pages_data.append({\n",
    "            'page_num': page_num + 1,\n",
    "            'text': text,\n",
    "            'section': section_title\n",
    "        })\n",
    "    \n",
    "    doc.close()\n",
    "    return pages_data\n",
    "\n",
    "def create_main_chunks(pages_data: List[Dict[str, Any]]) -> List[MainChunk]:\n",
    "    \"\"\"í˜ì´ì§€ ë°ì´í„°ë¥¼ í° êµ¬ì¡°í™”ëœ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.\"\"\"\n",
    "    chunks = []\n",
    "    current_section = \"\"\n",
    "    current_chunk_text = \"\"\n",
    "    current_pages = []\n",
    "    section_start_page = 1\n",
    "    \n",
    "    for i, page_data in enumerate(pages_data):\n",
    "        page_num = page_data['page_num']\n",
    "        text = page_data['text']\n",
    "        section = page_data['section']\n",
    "        \n",
    "        # ìƒˆë¡œìš´ ì„¹ì…˜ì´ ì‹œì‘ë˜ë©´ ì´ì „ ì²­í¬ë¥¼ ì™„ì„±\n",
    "        if section and section != current_section and current_chunk_text:\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            chunks.append(MainChunk(\n",
    "                text=current_chunk_text.strip(),\n",
    "                chunk_id=chunk_id,\n",
    "                page_start=section_start_page,\n",
    "                page_end=current_pages[-1] if current_pages else section_start_page,\n",
    "                section=current_section,\n",
    "                title=current_section\n",
    "            ))\n",
    "            \n",
    "            # ìƒˆ ì²­í¬ ì‹œì‘\n",
    "            current_chunk_text = text\n",
    "            current_section = section\n",
    "            current_pages = [page_num]\n",
    "            section_start_page = page_num\n",
    "        else:\n",
    "            # ê¸°ì¡´ ì²­í¬ì— í˜ì´ì§€ ì¶”ê°€\n",
    "            current_chunk_text += \"\\n\\n\" + text if current_chunk_text else text\n",
    "            current_pages.append(page_num)\n",
    "            if section and not current_section:\n",
    "                current_section = section\n",
    "                section_start_page = page_num\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€\n",
    "    if current_chunk_text.strip():\n",
    "        chunk_id = str(uuid.uuid4())\n",
    "        chunks.append(MainChunk(\n",
    "            text=current_chunk_text.strip(),\n",
    "            chunk_id=chunk_id,\n",
    "            page_start=section_start_page,\n",
    "            page_end=current_pages[-1] if current_pages else section_start_page,\n",
    "            section=current_section,\n",
    "            title=current_section\n",
    "        ))\n",
    "    \n",
    "    # ì²­í¬ê°€ ë„ˆë¬´ ì ìœ¼ë©´ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ë¶„í• \n",
    "    if len(chunks) < 3:\n",
    "        chunks = []\n",
    "        for page_data in pages_data:\n",
    "            chunk_id = str(uuid.uuid4())\n",
    "            chunks.append(MainChunk(\n",
    "                text=page_data['text'],\n",
    "                chunk_id=chunk_id,\n",
    "                page_start=page_data['page_num'],\n",
    "                page_end=page_data['page_num'],\n",
    "                section=page_data['section'],\n",
    "                title=f\"í˜ì´ì§€ {page_data['page_num']}\"\n",
    "            ))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def create_subchunks_from_main_chunk(main_chunk: MainChunk, target_tokens: int = 150) -> List[SubChunk]:\n",
    "    \"\"\"ë©”ì¸ ì²­í¬ì—ì„œ 2-3ë¬¸ì¥ ë‹¨ìœ„ì˜ ì„œë¸Œì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    subchunks = []\n",
    "    text = main_chunk.text\n",
    "    \n",
    "    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  (ê°œì„ ëœ ì •ê·œì‹)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Zê°€-í£])', text)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    current_subchunk = \"\"\n",
    "    current_tokens = 0\n",
    "    sentence_start_idx = 0\n",
    "    sentences_in_subchunk = 0\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "        \n",
    "        # 2-3ë¬¸ì¥ì´ê±°ë‚˜ í† í° ìˆ˜ê°€ ëª©í‘œì— ë„ë‹¬í•˜ë©´ ì„œë¸Œì²­í¬ ìƒì„±\n",
    "        should_create_subchunk = (\n",
    "            (sentences_in_subchunk >= 2 and current_tokens + sentence_tokens > target_tokens) or\n",
    "            sentences_in_subchunk >= 3 or\n",
    "            (current_tokens + sentence_tokens > target_tokens * 1.5 and sentences_in_subchunk >= 1)\n",
    "        )\n",
    "        \n",
    "        if should_create_subchunk and current_subchunk:\n",
    "            subchunk_id = str(uuid.uuid4())\n",
    "            subchunks.append(SubChunk(\n",
    "                text=current_subchunk.strip(),\n",
    "                subchunk_id=subchunk_id,\n",
    "                parent_chunk_id=main_chunk.chunk_id,\n",
    "                sentence_start=sentence_start_idx,\n",
    "                sentence_end=i - 1\n",
    "            ))\n",
    "            \n",
    "            # ìƒˆ ì„œë¸Œì²­í¬ ì‹œì‘\n",
    "            current_subchunk = sentence\n",
    "            current_tokens = sentence_tokens\n",
    "            sentence_start_idx = i\n",
    "            sentences_in_subchunk = 1\n",
    "        else:\n",
    "            # ê¸°ì¡´ ì„œë¸Œì²­í¬ì— ë¬¸ì¥ ì¶”ê°€\n",
    "            current_subchunk += \" \" + sentence if current_subchunk else sentence\n",
    "            current_tokens += sentence_tokens\n",
    "            sentences_in_subchunk += 1\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ ì„œë¸Œì²­í¬ ì¶”ê°€\n",
    "    if current_subchunk.strip():\n",
    "        subchunk_id = str(uuid.uuid4())\n",
    "        subchunks.append(SubChunk(\n",
    "            text=current_subchunk.strip(),\n",
    "            subchunk_id=subchunk_id,\n",
    "            parent_chunk_id=main_chunk.chunk_id,\n",
    "            sentence_start=sentence_start_idx,\n",
    "            sentence_end=len(sentences) - 1\n",
    "        ))\n",
    "    \n",
    "    return subchunks\n",
    "\n",
    "async def create_embeddings_batch(texts: List[str]) -> List[List[float]]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ ë°°ì¹˜ì— ëŒ€í•œ Gemini ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # Gemini ì„ë² ë”© ìƒì„± (langchain ì‚¬ìš©)\n",
    "        from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "        embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=GOOGLE_API_KEY)\n",
    "        embeddings = embeddings_model.embed_documents(texts)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "def create_bm25_index(subchunks: List[SubChunk]) -> BM25Okapi:\n",
    "    \"\"\"ì„œë¸Œì²­í¬ë“¤ì— ëŒ€í•œ BM25 ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\"\"\"\n",
    "    tokenized_subchunks = []\n",
    "    for subchunk in subchunks:\n",
    "        tokens = re.findall(r'\\b\\w+\\b', subchunk.text.lower())\n",
    "        tokenized_subchunks.append(tokens)\n",
    "    return BM25Okapi(tokenized_subchunks)\n",
    "\n",
    "\n",
    "@app.post(\"/upload-pdf\")\n",
    "async def upload_pdf(file: UploadFile = File(...)):\n",
    "    \"\"\"PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
    "    global collection, documents_registry, documents_chunks, documents_bm25\n",
    "    \n",
    "    if not file.filename.endswith('.pdf'):\n",
    "        raise HTTPException(status_code=400, detail=\"PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    try:\n",
    "        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
    "            content = await file.read()\n",
    "            tmp_file.write(content)\n",
    "            tmp_file_path = tmp_file.name\n",
    "        \n",
    "        # PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        logger.info(\"PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...\")\n",
    "        pages_data = extract_text_from_pdf(tmp_file_path)\n",
    "        \n",
    "        # ë©”ì¸ ì²­í¬ ìƒì„± (êµ¬ì¡°í™”ëœ í° ì²­í¬)\n",
    "        logger.info(\"êµ¬ì¡°í™”ëœ ë©”ì¸ ì²­í¬ ìƒì„± ì¤‘...\")\n",
    "        main_chunks = create_main_chunks(pages_data)\n",
    "        \n",
    "        if not main_chunks:\n",
    "            raise HTTPException(status_code=400, detail=\"PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ê° ë©”ì¸ ì²­í¬ì—ì„œ ì„œë¸Œì²­í¬ ìƒì„±\n",
    "        logger.info(\"ì„œë¸Œì²­í¬ ìƒì„± ì¤‘...\")\n",
    "        all_subchunks = []\n",
    "        for main_chunk in main_chunks:\n",
    "            subchunks = create_subchunks_from_main_chunk(main_chunk, target_tokens=150)\n",
    "            main_chunk.subchunks = subchunks\n",
    "            all_subchunks.extend(subchunks)\n",
    "        \n",
    "        if not all_subchunks:\n",
    "            raise HTTPException(status_code=400, detail=\"ì„œë¸Œì²­í¬ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ë¬¸ì„œ ID ìƒì„±\n",
    "        document_id = f\"doc_{uuid.uuid4().hex[:8]}_{int(time.time())}\"\n",
    "        \n",
    "        # ë¬¸ì„œë³„ ë°ì´í„° ì €ì¥\n",
    "        documents_chunks[document_id] = {\n",
    "            'main_chunks': main_chunks,\n",
    "            'subchunks': all_subchunks\n",
    "        }\n",
    "        \n",
    "        # BM25 ì¸ë±ìŠ¤ ìƒì„± (í•´ë‹¹ ë¬¸ì„œì˜ ì„œë¸Œì²­í¬ë§Œ)\n",
    "        logger.info(\"BM25 ì¸ë±ìŠ¤ ìƒì„± ì¤‘...\")\n",
    "        documents_bm25[document_id] = create_bm25_index(all_subchunks)\n",
    "        \n",
    "        # ChromaDB ì»¬ë ‰ì…˜ ìƒì„±/ì—…ë°ì´íŠ¸\n",
    "        collection_name = \"rag\"  # ê¸°ë³¸ ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "        \n",
    "        try:\n",
    "            collection = chroma_client.get_collection(collection_name)\n",
    "        except:\n",
    "            collection = chroma_client.create_collection(\n",
    "                name=collection_name,\n",
    "                metadata={\"hnsw:space\": \"cosine\"}\n",
    "            )\n",
    "        \n",
    "        # ì„œë¸Œì²­í¬ ì„ë² ë”© ìƒì„± ë° ì €ì¥ (ë°°ì¹˜ ì²˜ë¦¬)\n",
    "        logger.info(\"ì„œë¸Œì²­í¬ ì„ë² ë”© ìƒì„± ë° ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...\")\n",
    "        batch_size = 30  # OpenAI API ì œí•œì„ ê³ ë ¤í•œ ë°°ì¹˜ í¬ê¸°\n",
    "        \n",
    "        for i in range(0, len(all_subchunks), batch_size):\n",
    "            batch_subchunks = all_subchunks[i:i + batch_size]\n",
    "            batch_texts = [subchunk.text for subchunk in batch_subchunks]\n",
    "            \n",
    "            # ì„ë² ë”© ìƒì„±\n",
    "            embeddings = await create_embeddings_batch(batch_texts)\n",
    "            \n",
    "            # ê° ì„œë¸Œì²­í¬ì˜ ë¶€ëª¨ ì²­í¬ ì •ë³´ ì°¾ê¸°\n",
    "            metadata_list = []\n",
    "            for subchunk in batch_subchunks:\n",
    "                parent_chunk = next((chunk for chunk in main_chunks if chunk.chunk_id == subchunk.parent_chunk_id), None)\n",
    "                metadata_list.append({\n",
    "                    \"document_id\": document_id,\n",
    "                    \"subchunk_id\": subchunk.subchunk_id,\n",
    "                    \"parent_chunk_id\": subchunk.parent_chunk_id,\n",
    "                    \"parent_section\": parent_chunk.section if parent_chunk else \"\",\n",
    "                    \"parent_title\": parent_chunk.title if parent_chunk else \"\",\n",
    "                    \"page_start\": parent_chunk.page_start if parent_chunk else 0,\n",
    "                    \"page_end\": parent_chunk.page_end if parent_chunk else 0,\n",
    "                    \"sentence_start\": subchunk.sentence_start,\n",
    "                    \"sentence_end\": subchunk.sentence_end\n",
    "                })\n",
    "            \n",
    "            # ChromaDBì— ì €ì¥ (ë¬¸ì„œë³„ ê³ ìœ  ID ì‚¬ìš©)\n",
    "            collection.add(\n",
    "                documents=batch_texts,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadata_list,\n",
    "                ids=[f\"{document_id}_{subchunk.subchunk_id}\" for subchunk in batch_subchunks]\n",
    "            )\n",
    "        \n",
    "        # ë¬¸ì„œ ì •ë³´ë¥¼ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ì €ì¥\n",
    "        documents_registry[document_id] = {\n",
    "            \"document_id\": document_id,\n",
    "            \"filename\": file.filename,\n",
    "            \"upload_time\": time.time(),\n",
    "            \"main_chunks_count\": len(main_chunks),\n",
    "            \"subchunks_count\": len(all_subchunks),\n",
    "            \"total_pages\": len(pages_data),\n",
    "            \"chunks_info\": [\n",
    "                {\n",
    "                    \"chunk_id\": chunk.chunk_id,\n",
    "                    \"title\": chunk.title,\n",
    "                    \"section\": chunk.section,\n",
    "                    \"pages\": f\"{chunk.page_start}-{chunk.page_end}\",\n",
    "                    \"subchunks_count\": len(chunk.subchunks),\n",
    "                    \"text_preview\": chunk.text[:200] + \"...\" if len(chunk.text) > 200 else chunk.text\n",
    "                }\n",
    "                for chunk in main_chunks\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
    "        os.unlink(tmp_file_path)\n",
    "        \n",
    "        logger.info(f\"PDF ì²˜ë¦¬ ì™„ë£Œ: {len(main_chunks)}ê°œ ë©”ì¸ì²­í¬, {len(all_subchunks)}ê°œ ì„œë¸Œì²­í¬ ìƒì„±\")\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"message\": \"PDF ì—…ë¡œë“œ ë° ì²˜ë¦¬ ì™„ë£Œ\",\n",
    "            \"document_id\": document_id,\n",
    "            \"collection_name\": collection_name,\n",
    "            \"main_chunks_count\": len(main_chunks),\n",
    "            \"subchunks_count\": len(all_subchunks),\n",
    "            \"total_pages\": len(pages_data),\n",
    "            \"chunks_info\": documents_registry[document_id][\"chunks_info\"]\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        if 'tmp_file_path' in locals():\n",
    "            try:\n",
    "                os.unlink(tmp_file_path)\n",
    "            except:\n",
    "                pass\n",
    "        raise HTTPException(status_code=500, detail=f\"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "async def hybrid_search(query: str, document_id: str = None, vector_weight: float = 0.7, keyword_weight: float = 0.3, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„° + í‚¤ì›Œë“œ)ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if not collection:\n",
    "        raise HTTPException(status_code=400, detail=\"ë¨¼ì € PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "    \n",
    "    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "    query_embedding = await create_embeddings_batch([query])\n",
    "    query_embedding = query_embedding[0]\n",
    "    \n",
    "    # ë²¡í„° ê²€ìƒ‰ (ChromaDBì—ì„œ ì§ì ‘)\n",
    "    if document_id:\n",
    "        # íŠ¹ì • ë¬¸ì„œì—ì„œë§Œ ê²€ìƒ‰\n",
    "        vector_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=min(top_k * 3, 100),\n",
    "            where={\"document_id\": document_id}\n",
    "        )\n",
    "    else:\n",
    "        # ëª¨ë“  ë¬¸ì„œì—ì„œ ê²€ìƒ‰\n",
    "        vector_results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=min(top_k * 3, 100)\n",
    "        )\n",
    "    \n",
    "    if not vector_results['ids'][0]:\n",
    "        return []\n",
    "    \n",
    "    # BM25 ê²€ìƒ‰ì„ ìœ„í•œ ì¤€ë¹„\n",
    "    query_tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    \n",
    "    # ê²€ìƒ‰ ê²°ê³¼ ì²˜ë¦¬\n",
    "    search_results = []\n",
    "    \n",
    "    for i, (vector_id, distance, metadata, document_text) in enumerate(zip(\n",
    "        vector_results['ids'][0],\n",
    "        vector_results['distances'][0], \n",
    "        vector_results['metadatas'][0],\n",
    "        vector_results['documents'][0]\n",
    "    )):\n",
    "        # ë²¡í„° ì ìˆ˜ (ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜)\n",
    "        vector_score = 1 - distance\n",
    "        \n",
    "        # BM25 ì ìˆ˜ ê³„ì‚° (í•´ë‹¹ ë¬¸ì„œì˜ BM25 ì¸ë±ìŠ¤ ì‚¬ìš©)\n",
    "        doc_id = metadata.get('document_id', '')\n",
    "        bm25_score = 0.0\n",
    "        \n",
    "        if doc_id in documents_bm25 and document_text:\n",
    "            # ë¬¸ì„œ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”\n",
    "            doc_tokens = re.findall(r'\\b\\w+\\b', document_text.lower())\n",
    "            \n",
    "            # BM25 ì ìˆ˜ ê³„ì‚° (ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜)\n",
    "            query_term_scores = []\n",
    "            for term in query_tokens:\n",
    "                if term in doc_tokens:\n",
    "                    tf = doc_tokens.count(term)\n",
    "                    # ê°„ë‹¨í•œ BM25 ê·¼ì‚¬\n",
    "                    query_term_scores.append(tf / (tf + 1.0))\n",
    "            \n",
    "            if query_term_scores:\n",
    "                bm25_score = sum(query_term_scores) / len(query_term_scores)\n",
    "        \n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°\n",
    "        hybrid_score = (vector_weight * vector_score) + (keyword_weight * bm25_score)\n",
    "        \n",
    "        # ë¶€ëª¨ ì²­í¬ ì •ë³´ ì°¾ê¸°\n",
    "        parent_chunk = None\n",
    "        if doc_id in documents_chunks:\n",
    "            parent_chunk_id = metadata.get('parent_chunk_id', '')\n",
    "            for chunk in documents_chunks[doc_id]['main_chunks']:\n",
    "                if chunk.chunk_id == parent_chunk_id:\n",
    "                    parent_chunk = chunk\n",
    "                    break\n",
    "        \n",
    "        # ì„œë¸Œì²­í¬ ì •ë³´ ì°¾ê¸°\n",
    "        subchunk = None\n",
    "        if doc_id in documents_chunks:\n",
    "            subchunk_id = metadata.get('subchunk_id', '')\n",
    "            for sc in documents_chunks[doc_id]['subchunks']:\n",
    "                if sc.subchunk_id == subchunk_id:\n",
    "                    subchunk = sc\n",
    "                    break\n",
    "        \n",
    "        # ì„œë¸Œì²­í¬ê°€ ì—†ìœ¼ë©´ ì„ì‹œë¡œ ìƒì„±\n",
    "        if not subchunk:\n",
    "            subchunk = SubChunk(\n",
    "                text=document_text,\n",
    "                subchunk_id=metadata.get('subchunk_id', f'temp_{i}'),\n",
    "                parent_chunk_id=metadata.get('parent_chunk_id', ''),\n",
    "                sentence_start=0,\n",
    "                sentence_end=0\n",
    "            )\n",
    "        \n",
    "        search_results.append({\n",
    "            'subchunk': subchunk,\n",
    "            'parent_chunk': parent_chunk,\n",
    "            'score': hybrid_score,\n",
    "            'vector_score': vector_score,\n",
    "            'bm25_score': bm25_score,\n",
    "            'document_id': doc_id,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "    \n",
    "    # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬\n",
    "    search_results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    return search_results[:top_k]\n",
    "\n",
    "@app.post(\"/question\")\n",
    "async def ask_question(request: QuestionRequest):\n",
    "    \"\"\"ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        search_results = await hybrid_search(request.question, request.document_id, top_k=5)\n",
    "        \n",
    "        if not search_results:\n",
    "            raise HTTPException(status_code=404, detail=\"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        # ê²€ìƒ‰ëœ ì„œë¸Œì²­í¬ë“¤ê³¼ í•´ë‹¹í•˜ëŠ” ë¶€ëª¨ ì²­í¬ ì •ë³´ ìˆ˜ì§‘\n",
    "        context_info = []\n",
    "        used_parent_chunks = set()\n",
    "        \n",
    "        for result in search_results:\n",
    "            subchunk = result['subchunk']\n",
    "            parent_chunk = result['parent_chunk']\n",
    "            \n",
    "            # ë¶€ëª¨ ì²­í¬ì˜ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "            if parent_chunk and parent_chunk.chunk_id not in used_parent_chunks:\n",
    "                context_info.append({\n",
    "                    'parent_text': parent_chunk.text,\n",
    "                    'subchunk_text': subchunk.text,\n",
    "                    'title': parent_chunk.title,\n",
    "                    'section': parent_chunk.section,\n",
    "                    'page_start': parent_chunk.page_start,\n",
    "                    'page_end': parent_chunk.page_end,\n",
    "                    'score': result['score'],\n",
    "                    'document_id': result['document_id']\n",
    "                })\n",
    "                used_parent_chunks.add(parent_chunk.chunk_id)\n",
    "            elif not parent_chunk:\n",
    "                # ë¶€ëª¨ ì²­í¬ê°€ ì—†ëŠ” ê²½ìš° ì„œë¸Œì²­í¬ë§Œ ì‚¬ìš©\n",
    "                context_info.append({\n",
    "                    'parent_text': subchunk.text,\n",
    "                    'subchunk_text': subchunk.text,\n",
    "                    'title': result['metadata'].get('parent_title', 'ì œëª© ì—†ìŒ'),\n",
    "                    'section': result['metadata'].get('parent_section', 'ì„¹ì…˜ ì—†ìŒ'),\n",
    "                    'page_start': result['metadata'].get('page_start', 0),\n",
    "                    'page_end': result['metadata'].get('page_end', 0),\n",
    "                    'score': result['score'],\n",
    "                    'document_id': result['document_id']\n",
    "                })\n",
    "        \n",
    "        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± (ë¶€ëª¨ ì²­í¬ì˜ ì „ì²´ í…ìŠ¤íŠ¸ ì‚¬ìš©)\n",
    "        context_parts = []\n",
    "        for info in context_info:\n",
    "            context_part = f\"[{info['title']} - í˜ì´ì§€ {info['page_start']}\"\n",
    "            if info['page_end'] != info['page_start']:\n",
    "                context_part += f\"-{info['page_end']}\"\n",
    "            context_part += f\", ì„¹ì…˜: {info['section']}]\\n{info['parent_text']}\"\n",
    "            context_parts.append(context_part)\n",
    "        \n",
    "        context = \"\\n\\n\" + \"=\"*50 + \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "        prompt = f\"\"\"ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ë¬¸ì„œì˜ ê´€ë ¨ ì„¹ì…˜ë“¤ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”:\n",
    "\n",
    "=== ê´€ë ¨ ë¬¸ì„œ ì„¹ì…˜ë“¤ ===\n",
    "{context}\n",
    "\n",
    "=== ì‚¬ìš©ì ì§ˆë¬¸ ===\n",
    "{request.question}\n",
    "\n",
    "=== ë‹µë³€ ì§€ì¹¨ ===\n",
    "1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”\n",
    "2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”\n",
    "3. ê´€ë ¨ ì„¹ì…˜ê³¼ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ì–¸ê¸‰í•´ì£¼ì„¸ìš”\n",
    "4. ì—¬ëŸ¬ ì„¹ì…˜ì˜ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”\n",
    "5. ë‹µë³€ì´ ë¶ˆë¶„ëª…í•˜ë‹¤ë©´ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”\n",
    "6. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "\n",
    "        # Gemini API í˜¸ì¶œ\n",
    "        response = llm.invoke(prompt)\n",
    "        answer = response.content\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"question\": request.question,\n",
    "            \"answer\": answer,\n",
    "            \"context_sources\": [\n",
    "                {\n",
    "                    \"document_id\": info['document_id'],\n",
    "                    \"title\": info['title'],\n",
    "                    \"section\": info['section'],\n",
    "                    \"pages\": f\"{info['page_start']}-{info['page_end']}\" if info['page_end'] != info['page_start'] else str(info['page_start']),\n",
    "                    \"score\": round(info['score'], 3),\n",
    "                    \"matched_subchunk\": info['subchunk_text'][:200] + \"...\" if len(info['subchunk_text']) > 200 else info['subchunk_text'],\n",
    "                    \"full_section_preview\": info['parent_text'][:300] + \"...\" if len(info['parent_text']) > 300 else info['parent_text']\n",
    "                }\n",
    "                for info in context_info\n",
    "            ],\n",
    "            \"total_sections_found\": len(context_info)\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ì§ˆë¬¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "@app.delete(\"/delete-document/{document_id}\")\n",
    "async def delete_document(document_id: str):\n",
    "    \"\"\"íŠ¹ì • ë¬¸ì„œì™€ ê´€ë ¨ëœ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    global collection, documents_registry, documents_chunks, documents_bm25\n",
    "    \n",
    "    try:\n",
    "        deleted_items = []\n",
    "        \n",
    "        # 1. ë¬¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ì‚­ì œ\n",
    "        if document_id in documents_registry:\n",
    "            del documents_registry[document_id]\n",
    "            deleted_items.append(\"ë¬¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì •ë³´\")\n",
    "        \n",
    "        # 2. ë¬¸ì„œë³„ ì²­í¬ ë°ì´í„° ì‚­ì œ\n",
    "        if document_id in documents_chunks:\n",
    "            chunk_count = len(documents_chunks[document_id]['main_chunks'])\n",
    "            subchunk_count = len(documents_chunks[document_id]['subchunks'])\n",
    "            del documents_chunks[document_id]\n",
    "            deleted_items.append(f\"ë©”ì¸ ì²­í¬ ë°ì´í„° ({chunk_count}ê°œ)\")\n",
    "            deleted_items.append(f\"ì„œë¸Œ ì²­í¬ ë°ì´í„° ({subchunk_count}ê°œ)\")\n",
    "        \n",
    "        # 3. ë¬¸ì„œë³„ BM25 ì¸ë±ìŠ¤ ì‚­ì œ\n",
    "        if document_id in documents_bm25:\n",
    "            del documents_bm25[document_id]\n",
    "            deleted_items.append(\"BM25 ì¸ë±ìŠ¤\")\n",
    "        \n",
    "        # 4. ChromaDBì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ë°ì´í„°ë§Œ ì‚­ì œ\n",
    "        try:\n",
    "            if collection:\n",
    "                # í•´ë‹¹ ë¬¸ì„œì˜ ëª¨ë“  ë²¡í„° ë°ì´í„° ì‚­ì œ\n",
    "                collection.delete(where={\"document_id\": document_id})\n",
    "                deleted_items.append(f\"ë²¡í„° ë°ì´í„° (document_id: {document_id})\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ChromaDBì—ì„œ ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        logger.info(f\"ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ: {document_id}, ì‚­ì œëœ í•­ëª©: {deleted_items}\")\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"message\": f\"ë¬¸ì„œ '{document_id}' ì‚­ì œ ì™„ë£Œ\",\n",
    "            \"document_id\": document_id,\n",
    "            \"deleted_items\": deleted_items,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "@app.delete(\"/delete-all\")\n",
    "async def delete_all_documents():\n",
    "    \"\"\"ëª¨ë“  ë¬¸ì„œì™€ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.\"\"\"\n",
    "    global collection, documents_registry, documents_chunks, documents_bm25\n",
    "    \n",
    "    try:\n",
    "        deleted_items = []\n",
    "        \n",
    "        # 1. ë¬¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì´ˆê¸°í™”\n",
    "        doc_count = len(documents_registry)\n",
    "        documents_registry.clear()\n",
    "        deleted_items.append(f\"ë¬¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ({doc_count}ê°œ ë¬¸ì„œ)\")\n",
    "        \n",
    "        # 2. ë¬¸ì„œë³„ ì²­í¬ ë°ì´í„° ì´ˆê¸°í™”\n",
    "        chunk_doc_count = len(documents_chunks)\n",
    "        documents_chunks.clear()\n",
    "        deleted_items.append(f\"ë¬¸ì„œë³„ ì²­í¬ ë°ì´í„° ({chunk_doc_count}ê°œ ë¬¸ì„œ)\")\n",
    "        \n",
    "        # 3. ë¬¸ì„œë³„ BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™”\n",
    "        bm25_doc_count = len(documents_bm25)\n",
    "        documents_bm25.clear()\n",
    "        deleted_items.append(f\"ë¬¸ì„œë³„ BM25 ì¸ë±ìŠ¤ ({bm25_doc_count}ê°œ ë¬¸ì„œ)\")\n",
    "        \n",
    "        # 4. ChromaDB rag ì»¬ë ‰ì…˜ ì´ˆê¸°í™”\n",
    "        try:\n",
    "            if collection:\n",
    "                # ì»¬ë ‰ì…˜ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ\n",
    "                all_data = collection.get()\n",
    "                if all_data['ids']:\n",
    "                    collection.delete(ids=all_data['ids'])\n",
    "                    deleted_items.append(f\"rag ì»¬ë ‰ì…˜ ë°ì´í„° ({len(all_data['ids'])}ê°œ)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ChromaDB ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        # 5. ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬\n",
    "        temp_dir = tempfile.gettempdir()\n",
    "        temp_files_deleted = 0\n",
    "        try:\n",
    "            for filename in os.listdir(temp_dir):\n",
    "                if filename.endswith('.pdf') and 'tmp' in filename:\n",
    "                    temp_file_path = os.path.join(temp_dir, filename)\n",
    "                    try:\n",
    "                        os.unlink(temp_file_path)\n",
    "                        temp_files_deleted += 1\n",
    "                    except:\n",
    "                        pass\n",
    "            if temp_files_deleted > 0:\n",
    "                deleted_items.append(f\"ì„ì‹œ PDF íŒŒì¼ë“¤ ({temp_files_deleted}ê°œ)\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        logger.info(f\"ëª¨ë“  ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ, ì‚­ì œëœ í•­ëª©: {deleted_items}\")\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"message\": \"ëª¨ë“  ë¬¸ì„œì™€ ë°ì´í„° ì‚­ì œ ì™„ë£Œ\",\n",
    "            \"deleted_items\": deleted_items,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ì „ì²´ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ì „ì²´ ì‚­ì œ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "@app.get(\"/documents\")\n",
    "async def list_documents():\n",
    "    \"\"\"í˜„ì¬ ì‹œìŠ¤í…œì— ë“±ë¡ëœ ëª¨ë“  ë¬¸ì„œ ëª©ë¡ì„ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        # ChromaDBì—ì„œ ì‹¤ì œ ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸\n",
    "        vector_count = 0\n",
    "        unique_documents = set()\n",
    "        \n",
    "        if collection:\n",
    "            try:\n",
    "                all_data = collection.get()\n",
    "                vector_count = len(all_data['ids']) if all_data['ids'] else 0\n",
    "                \n",
    "                # ë¬¸ì„œë³„ ë²¡í„° ìˆ˜ ê³„ì‚°\n",
    "                for metadata in all_data['metadatas'] or []:\n",
    "                    if 'document_id' in metadata:\n",
    "                        unique_documents.add(metadata['document_id'])\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ChromaDB ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        # ë¬¸ì„œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì •ë³´ì™€ ì‹¤ì œ ë²¡í„°DB ì •ë³´ ê²°í•©\n",
    "        documents_list = []\n",
    "        for doc_id, doc_info in documents_registry.items():\n",
    "            # ì‹¤ì œ ë²¡í„°DBì—ì„œ í•´ë‹¹ ë¬¸ì„œì˜ ë²¡í„° ìˆ˜ í™•ì¸\n",
    "            actual_vectors = 0\n",
    "            if collection:\n",
    "                try:\n",
    "                    doc_vectors = collection.get(where={\"document_id\": doc_id})\n",
    "                    actual_vectors = len(doc_vectors['ids']) if doc_vectors['ids'] else 0\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            documents_list.append({\n",
    "                **doc_info,\n",
    "                \"actual_vectors_count\": actual_vectors,\n",
    "                \"upload_time_formatted\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(doc_info['upload_time']))\n",
    "            })\n",
    "        \n",
    "        return JSONResponse({\n",
    "            \"total_documents\": len(documents_registry),\n",
    "            \"total_vectors\": vector_count,\n",
    "            \"unique_documents_in_db\": len(unique_documents),\n",
    "            \"collection_name\": \"rag\",\n",
    "            \"current_memory_status\": {\n",
    "                \"documents_chunks_loaded\": len(documents_chunks),\n",
    "                \"documents_bm25_loaded\": len(documents_bm25),\n",
    "                \"vector_db_ready\": collection is not None\n",
    "            },\n",
    "            \"documents\": documents_list\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "@app.get(\"/document/{document_id}\")\n",
    "async def get_document_detail(document_id: str):\n",
    "    \"\"\"íŠ¹ì • ë¬¸ì„œì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.\"\"\"\n",
    "    try:\n",
    "        if document_id not in documents_registry:\n",
    "            raise HTTPException(status_code=404, detail=f\"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_id}\")\n",
    "        \n",
    "        doc_info = documents_registry[document_id].copy()\n",
    "        \n",
    "        # ChromaDBì—ì„œ ì‹¤ì œ ë²¡í„° ë°ì´í„° í™•ì¸\n",
    "        actual_vectors = 0\n",
    "        vector_samples = []\n",
    "        \n",
    "        if collection:\n",
    "            try:\n",
    "                doc_vectors = collection.get(\n",
    "                    where={\"document_id\": document_id},\n",
    "                    limit=5  # ìƒ˜í”Œ 5ê°œë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "                )\n",
    "                actual_vectors = len(doc_vectors['ids']) if doc_vectors['ids'] else 0\n",
    "                \n",
    "                # ë²¡í„° ìƒ˜í”Œ ì •ë³´\n",
    "                if doc_vectors['documents']:\n",
    "                    for i, (doc_text, metadata) in enumerate(zip(doc_vectors['documents'], doc_vectors['metadatas'] or [])):\n",
    "                        vector_samples.append({\n",
    "                            \"sample_id\": i + 1,\n",
    "                            \"text_preview\": doc_text[:150] + \"...\" if len(doc_text) > 150 else doc_text,\n",
    "                            \"parent_section\": metadata.get('parent_section', ''),\n",
    "                            \"page_range\": f\"{metadata.get('page_start', '')}-{metadata.get('page_end', '')}\"\n",
    "                        })\n",
    "                        \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"ë¬¸ì„œ ë²¡í„° ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        doc_info.update({\n",
    "            \"actual_vectors_count\": actual_vectors,\n",
    "            \"upload_time_formatted\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(doc_info['upload_time'])),\n",
    "            \"vector_samples\": vector_samples\n",
    "        })\n",
    "        \n",
    "        return JSONResponse(doc_info)\n",
    "    \n",
    "    except HTTPException:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"API ìƒíƒœ í™•ì¸\"\"\"\n",
    "    return {\n",
    "        \"message\": \"PDF QA System API\",\n",
    "        \"status\": \"running\",\n",
    "        \"endpoints\": {\n",
    "            \"upload\": \"/upload-pdf\",\n",
    "            \"question\": \"/question\",\n",
    "            \"delete_document\": \"/delete-document/{document_id}\",\n",
    "            \"delete_all\": \"/delete-all\",\n",
    "            \"list_documents\": \"/documents\",\n",
    "            \"document_detail\": \"/document/{document_id}\",\n",
    "            \"status\": \"/status\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/status\")\n",
    "async def get_status():\n",
    "    \"\"\"í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸\"\"\"\n",
    "    total_main_chunks = sum(len(data['main_chunks']) for data in documents_chunks.values())\n",
    "    total_subchunks = sum(len(data['subchunks']) for data in documents_chunks.values())\n",
    "    \n",
    "    return {\n",
    "        \"total_documents\": len(documents_registry),\n",
    "        \"documents_chunks_loaded\": len(documents_chunks),\n",
    "        \"documents_bm25_loaded\": len(documents_bm25),\n",
    "        \"total_main_chunks\": total_main_chunks,\n",
    "        \"total_subchunks\": total_subchunks,\n",
    "        \"vector_db_ready\": collection is not None,\n",
    "        \"documents_info\": [\n",
    "            {\n",
    "                \"document_id\": doc_id,\n",
    "                \"filename\": doc_info[\"filename\"],\n",
    "                \"main_chunks\": len(documents_chunks[doc_id]['main_chunks']) if doc_id in documents_chunks else 0,\n",
    "                \"subchunks\": len(documents_chunks[doc_id]['subchunks']) if doc_id in documents_chunks else 0\n",
    "            }\n",
    "            for doc_id, doc_info in documents_registry.items()\n",
    "        ]\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8080)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
