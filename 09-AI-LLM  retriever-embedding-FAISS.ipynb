{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6a489e5",
   "metadata": {},
   "source": [
    "## 1. 지식기반 데이터를 임베딩하여 벡터DB에 저장 후 Retriever 예졔\n",
    "\n",
    "- **실제 Google Gemini(Google Generative AI) 생태계 기반으로**\n",
    "  - 임베딩 생성\n",
    "  - 벡터 DB(FAISS) 검색\n",
    "  - LLM 응답 생성\n",
    "  을 **통합 파이프라인**으로 구현하는 방법을 실습한다.\n",
    "- RAG(Retrieval-Augmented Generation)의 전체 흐름을  \n",
    "  최신 오픈소스와 Google의 공식 AI API로 직접 체험할 수 있도록 한다.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 코드 핵심 내용 및 주요 포인트\n",
    "\n",
    "### 2.1 Gemini 임베딩과 벡터스토어 생성\n",
    "\n",
    "- `GoogleGenerativeAIEmbeddings`를 이용해 텍스트(문서/지식베이스)를 **의미 벡터**로 변환\n",
    "- `FAISS` 벡터스토어에 저장하여 **유사도 검색**이 가능하도록 구성\n",
    "\n",
    "### 2.2 RAG 프롬프트 템플릿\n",
    "\n",
    "- 검색 결과로 추출된 문서(컨텍스트)와 사용자의 질문을\n",
    "  하나의 템플릿으로 LLM에 전달\n",
    "\n",
    "### 2.3 Gemini LLM 연동\n",
    "\n",
    "- `ChatGoogleGenerativeAI`를 통해 Gemini 최신 LLM을 사용  \n",
    "  (Gemini Flash, Pro 등)\n",
    "- 프롬프트에 기반한 **정확하고 맥락 반영된 답변 생성**\n",
    "\n",
    "### 2.4 LCEL 기반 체인 자동화\n",
    "\n",
    "- **Retriever → 컨텍스트 포맷팅 → 프롬프트 → LLM → 출력 파싱**을  \n",
    "  파이프라인(LCEL) 한 줄로 연결\n",
    "- 복잡한 RAG 작업을 코드 최소화로 자동화\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 실무적 교육 메시지\n",
    "\n",
    "- **RAG(검색 증강 생성) 파이프라인의 핵심 원리**:  \n",
    "  검색 + 생성이 결합된 고도화된 AI 워크플로우의 표준\n",
    "- **Gemini 활용법 실습**:  \n",
    "  구글 공식 임베딩/LLM API와 최신 LangChain 활용 실전 노하우\n",
    "- **확장성/자동화**:  \n",
    "  실무에서 대규모 지식베이스, 엔터프라이즈 챗봇, 문서 Q&A 등에  \n",
    "  즉시 응용 가능한 기술 구조를 익힘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 환경변수 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 지식베이스 문서들\n",
    "knowledge_base = [\n",
    "    \"LangChain은 2022년에 Harrison Chase가 개발한 프레임워크입니다.\",\n",
    "    \"LangChain의 주요 구성요소는 LLM, 프롬프트, 메모리, 체인, 에이전트입니다.\",\n",
    "    \"체인은 여러 구성요소를 연결하여 복잡한 작업을 수행합니다.\",\n",
    "    \"에이전트는 도구를 사용하여 동적으로 작업을 수행할 수 있습니다.\"\n",
    "]\n",
    "\n",
    "# Gemini 임베딩 벡터 저장소 생성\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"\n",
    ")\n",
    "vectorstore = FAISS.from_texts(knowledge_base, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e767a6",
   "metadata": {},
   "source": [
    "## retriever = vectorstore.as_retriever()의 의미\n",
    "\n",
    "### 1. 정의 및 목적\n",
    "\n",
    "- **vectorstore.as_retriever()**는  \n",
    "  벡터DB(예: FAISS)에 저장된 임베딩 기반 문서들을  \n",
    "  **\"검색기(Retriever)\" 객체로 변환**하는 함수입니다.\n",
    "- Retriever는 **입력 쿼리(질문 등)**를 받아  \n",
    "  **의미적으로 가장 유사한 문서들을 자동으로 찾아 반환**하는 역할을 합니다.\n",
    "\n",
    "### 2. 왜 필요한가?\n",
    "\n",
    "- **RAG(검색 증강 생성) 워크플로우**에서  \n",
    "  LLM에게 컨텍스트(문맥)을 제공하기 위해  \n",
    "  반드시 \"질문 → 문서 검색 → 문서 전달\" 절차가 필요합니다.\n",
    "- `as_retriever()`를 통해  \n",
    "  벡터DB를 LLM/RAG 파이프라인과 바로 연결할 수 있습니다.\n",
    "\n",
    "### 3. 실무적 역할\n",
    "\n",
    "- **vectorstore(FAISS 등)는**  \n",
    "  문서 저장 및 벡터 검색 API를 모두 포함\n",
    "- **Retriever로 변환하면:**  \n",
    "  - LangChain 표준 체인(LCEL) 및 RAG 파이프라인에 **직접 연결** 가능\n",
    "  - 입력 쿼리를 받아 자동으로 유사도 검색을 수행\n",
    "\n",
    "### 4. 활용 예시\n",
    "\n",
    "```python\n",
    "retriever = vectorstore.as_retriever()\n",
    "top_docs = retriever.invoke(\"질문 또는 쿼리\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75619eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 프롬프트 템플릿\n",
    "template = \"\"\"다음 컨텍스트를 기반으로 질문에 답해주세요:\n",
    "\n",
    "{context}\n",
    "\n",
    "질문: {question}\n",
    "답변:\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Gemini LLM 초기화\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",   # 또는 gemini-pro 등 최신 모델명 사용 가능\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# RAG 체인 구성 (LCEL)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 질문하기\n",
    "question = \"LangChain의 주요 구성요소는 무엇인가요?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"질문: {question}\")\n",
    "print(f\"답변: {answer}\")\n",
    "\n",
    "# 추가 질문\n",
    "question2 = \"체인은 무엇을 하나요?\"\n",
    "answer2 = rag_chain.invoke(question2)\n",
    "print(f\"\\n질문: {question2}\")\n",
    "print(f\"답변: {answer2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c80398",
   "metadata": {},
   "source": [
    "### `format_docs(docs)` 함수의 의미\n",
    "\n",
    "---\n",
    "\n",
    "### 1. 함수 목적\n",
    "\n",
    "- **복수 개의 문서(청크)**를  \n",
    "  하나의 **긴 텍스트(문자열)**로 합치는 역할을 한다.\n",
    "- 주로 **RAG(검색 증강 생성) 파이프라인에서,**  \n",
    "  검색된 여러 문서의 내용을 LLM 프롬프트에  \n",
    "  \"컨텍스트\"로 전달할 때 사용한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 동작 구조\n",
    "\n",
    "```python\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d7484",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-lecture-8YS9sgva-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
